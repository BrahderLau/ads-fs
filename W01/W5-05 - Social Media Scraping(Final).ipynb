{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W5-05 - Social Media Scraping\n",
    "\n",
    "## Exercise(s): Learn how to perform Data Scraping\n",
    "\n",
    "\n",
    "Objective: Learn to scrape Facebook, Instagram, Twitters, Reddit (FITR)\n",
    "\n",
    "Competencies:\n",
    "-\tParticipants will be able to scrape Facebook, Instagram, Twitters, Reddit (FITR)\n",
    "\n",
    "Tools: Python, Anaconda, Jupyter\n",
    "\n",
    "Analysis case study: -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra note for yesterday\n",
    "\n",
    "What is a User Agent?\n",
    "In computing, a user agent is anything that is acting on behalf of the user. A Borwser's User agent is a string or line of text, containing information about the browser and operating system, which is sent to the web server in the HTTP request header. This allows the website to customize the content, to best suit the capabilities of the particular device, but sharing this information also raises certain privacy issues.\n",
    "\n",
    "In a typical HTTP request/response cycle you can view the browser's User-Agent inside the HTTP request/response header. It looks something like:\n",
    "\n",
    "![changing-user-agent-1](changing-user-agent-1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook_scraper\n",
    "\n",
    "Install Command:\n",
    "- pip install facebook-scraper\n",
    "\n",
    "Description:\n",
    "- Allows you to scrape public facebook pages without an API Key\n",
    "\n",
    "Optional parameters:\n",
    "- group: group id, to scrape groupHi s instead of pages. Default is None.\n",
    "- pages: how many pages of posts to request, usually the first page has 2 posts and the rest 4. Default is 10.\n",
    "- timeout: how many seconds to wait before timing out. Default is 5.\n",
    "- credentials: tuple of user and password to login before requesting the posts. Default is None.\n",
    "- extra_info: bool, if true the function will try to do an extra request to get the post reactions. Default is False.\n",
    "- youtube_dl: bool, use Youtube-DL for (high-quality) video extraction. You need to have youtube-dl installed on your environment. Default is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from facebook_scraper import get_posts\n",
    "\n",
    "for post in get_posts('nintendo', pages=1):\n",
    "    print(post['text'][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample data returned\n",
    "\n",
    "{'post_id': '2257188721032235',\n",
    " 'text': 'Don’t let this diminutive version of the Hero of Time fool you, '\n",
    "         'Young Link is just as heroic as his fully grown version! Young Link '\n",
    "         'joins the Super Smash Bros. series of amiibo figures!',\n",
    " 'time': datetime.datetime(2019, 4, 29, 12, 0, 1),\n",
    " 'image': 'https://scontent.flim16-1.fna.fbcdn.net'\n",
    "          '/v/t1.0-0/cp0/e15/q65/p320x320'\n",
    "          '/58680860_2257182054366235_1985558733786185728_n.jpg'\n",
    "          '?_nc_cat=1&_nc_ht=scontent.flim16-1.fna'\n",
    "          '&oh=31b0ba32ec7886e95a5478c479ba1d38&oe=5D6CDEE4',\n",
    " 'images': ['https://scontent.flim16-1.fna.fbcdn.net'\n",
    "          '/v/t1.0-0/cp0/e15/q65/p320x320'\n",
    "          '/58680860_2257182054366235_1985558733786185728_n.jpg'\n",
    "          '?_nc_cat=1&_nc_ht=scontent.flim16-1.fna'\n",
    "          '&oh=31b0ba32ec7886e95a5478c479ba1d38&oe=5D6CDEE4'],\n",
    " 'likes': 2036,\n",
    " 'comments': 214,\n",
    " 'shares': 0,\n",
    " 'reactions': {'like': 135, 'love': 64, 'haha': 10, 'wow': 4, 'anger': 1},  # if `extra_info` was set\n",
    " 'post_url': 'https://m.facebook.com/story.php'\n",
    "             '?story_fbid=2257188721032235&id=119240841493711',\n",
    " 'link': 'https://bit.ly/something'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Download comments for a public Facebook post.\n",
    "# \"\"\"\n",
    "\n",
    "# import facebook_scraper as fs\n",
    "\n",
    "# # get POST_ID from the URL of the post which can have the following structure:\n",
    "# # https://www.facebook.com/USER/posts/POST_ID\n",
    "# # https://www.facebook.com/groups/GROUP_ID/posts/POST_ID\n",
    "# POST_ID = \"pfbid02NsuAiBU9o1ouwBrw1vYAQ7khcVXvz8F8zMvkVat9UJ6uiwdgojgddQRLpXcVBqYbl\"\n",
    "\n",
    "# # number of comments to download\n",
    "# MAX_COMMENTS = 2\n",
    "\n",
    "# # get the post (this gives a generator)\n",
    "# gen = fs.get_posts(\n",
    "#     post_urls=[POST_ID],\n",
    "#     options={\"comments\": MAX_COMMENTS, \"progress\": True}\n",
    "# )\n",
    "\n",
    "# # take 1st element of the generator which is the post we requested\n",
    "# post = next(gen)\n",
    "\n",
    "# # extract the comments part\n",
    "# comments = post['comments_full']\n",
    "\n",
    "# # process comments as you want...\n",
    "# for comment in comments:\n",
    "\n",
    "#     # e.g. ...print them\n",
    "#     print(comment)\n",
    "\n",
    "#     # e.g. ...get the replies for them\n",
    "#     for reply in comment['replies']:\n",
    "#         print(' ', reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from facebook_scraper import get_profile\n",
    "# get_profile(\"zuck\") # Or get_profile(\"zuck\", cookies=\"cookies.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for post in get_posts('Nintendo', pages=10):\n",
    "    print(post['text'][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facebook_scraper import get_posts\n",
    "\n",
    "group_id = '797518550594060' #DeveloperKaki\n",
    "username = 'khlau_1012@hotmail.com'\n",
    "password = 'your_password'\n",
    "\n",
    "for post in get_posts(group=group_id, pages=1, credentials=(username , password)):\n",
    "    print(post['text'][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Friend_count': None,\n",
       " 'Follower_count': None,\n",
       " 'Following_count': None,\n",
       " 'cover_photo': 'https://scontent-kul2-1.xx.fbcdn.net/v/t31.18172-8/19575079_10103832396388711_8894816584589808440_o.jpg?stp=cp0_dst-jpg_e15_fr_q65&_nc_cat=109&ccb=1-7&_nc_sid=d668b3&_nc_ohc=J-cM3PlmuMgAX_USb2F&_nc_ht=scontent-kul2-1.xx&oh=00_AfB4iAoBhDPdfFSTYNa4fygGyamPVJfVVjr_-nO2QzW9Pw&oe=65C9727D',\n",
       " 'profile_picture': 'https://scontent-kul2-1.xx.fbcdn.net/v/t39.30808-1/312257846_10114737758665291_6588360857015169674_n.jpg?stp=cp0_dst-jpg_e15_q65_s120x120&_nc_cat=1&ccb=1-7&_nc_sid=4da83f&_nc_ohc=P9djs7GevxAAX8qSBNc&_nc_ht=scontent-kul2-1.xx&oh=00_AfDu4htlN-5cpvpWw2-K2e9LEhAaDwKEsAujmPR8TnE93w&oe=65A6541E',\n",
       " 'id': '4',\n",
       " 'Name': 'Mark Zuckerberg',\n",
       " 'ที่ทำงาน': 'Chan Zuckerberg Initiative\\n1 ธันวาคม 2015 - ปัจจุบัน\\nMeta\\nFounder and CEO\\n4 กุมภาพันธ์ 2004 - ปัจจุบัน\\nPalo Alto, California\\nBringing the world closer together.',\n",
       " 'การศึกษา': 'Harvard University\\nComputer Science and Psychology\\n30 สิงหาคม 2002 - 30 เมษายน 2004\\nPhillips Exeter Academy\\nClassics\\nจบการศึกษาเมื่อปี 2002\\nArdsley High School\\nโรงเรียนมัธยม\\nสิงหาคม 1998 - พฤษภาคม 2000',\n",
       " 'สถานที่ที่เคยอาศัยอยู่': 'Palo Alto, California\\nเมืองปัจจุบัน\\nDobbs Ferry, New York\\nเมืองเกิด',\n",
       " 'เกี่ยวกับ Mark': \"I'm trying to make the world a more open place.\",\n",
       " 'คำคมที่ชอบ': '\"Fortune favors the bold.\"\\n- Virgil, Aeneid X.284\\n\\n\"All children are artists. The problem is how to remain an artist once you grow up.\"\\n- Pablo Picasso\\n\\n\"Make things as simple as possible but no simpler.\"\\n- Albert Einstein'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from facebook_scraper import get_profile\n",
    "username = 'khlau_1012@hotmail.com'\n",
    "password = 'your_password'\n",
    "get_profile(\"zuck\", credentials=(username , password))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Resources:\n",
    "- [facebook-post-scraper](https://github.com/brutalsavage/facebook-post-scraper). Has comments. Uses Selenium.\n",
    "- [facebook-scraper-selenium](https://github.com/apurvmishra99/facebook-scraper-selenium). \"Scrape posts from any group or user into a .csv file without needing to - register for any API access\".\n",
    "- [Ultimate Facebook Scraper](https://github.com/harismuneer/Ultimate-Facebook-Scraper). \"Scrapes almost everything about a Facebook user's profile\". Uses Selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 : tweepy\n",
    "\n",
    "Install Command:\n",
    "- !pip install tweepy\n",
    "\n",
    "Description:\n",
    "- Allows you to scrape tweets and replies\n",
    "\n",
    "Info List:\n",
    "![tweepy_info](tweepy_info.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-4.14.0-py3-none-any.whl (98 kB)\n",
      "     ---------------------------------------- 0.0/98.5 kB ? eta -:--:--\n",
      "     ---- ----------------------------------- 10.2/98.5 kB ? eta -:--:--\n",
      "     ---- ----------------------------------- 10.2/98.5 kB ? eta -:--:--\n",
      "     ----------- -------------------------- 30.7/98.5 kB 187.9 kB/s eta 0:00:01\n",
      "     ----------- -------------------------- 30.7/98.5 kB 187.9 kB/s eta 0:00:01\n",
      "     --------------- ---------------------- 41.0/98.5 kB 151.3 kB/s eta 0:00:01\n",
      "     --------------- ---------------------- 41.0/98.5 kB 151.3 kB/s eta 0:00:01\n",
      "     ----------------------- -------------- 61.4/98.5 kB 182.2 kB/s eta 0:00:01\n",
      "     ----------------------------------- -- 92.2/98.5 kB 228.2 kB/s eta 0:00:01\n",
      "     -------------------------------------- 98.5/98.5 kB 245.9 kB/s eta 0:00:00\n",
      "Collecting oauthlib<4,>=3.2.0 (from tweepy)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "     ------------- ------------------------- 51.2/151.7 kB 1.3 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 92.2/151.7 kB 1.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ---- 133.1/151.7 kB 1.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ---- 133.1/151.7 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- - 143.4/151.7 kB 607.9 kB/s eta 0:00:01\n",
      "     ------------------------------------ 151.7/151.7 kB 645.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from tweepy) (2.31.0)\n",
      "Collecting requests-oauthlib<2,>=1.2.0 (from tweepy)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2023.11.17)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.2.2 requests-oauthlib-1.3.1 tweepy-4.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_token = \"\"\n",
    "access_token_secret = \"\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scraping a specific Twitter user’s Tweets:\n",
    "username = 'jack'  # username handle\n",
    "count = 150        # recent 150 tweets\n",
    "\n",
    "try:     \n",
    " # Creation of query method using parameters\n",
    " tweets = tweepy.Cursor(api.user_timeline,id=username).items(count)\n",
    " \n",
    " # Pulling information from tweets iterable object\n",
    " tweets_list = [[tweet.created_at, tweet.id, tweet.text] for tweet in tweets]\n",
    " \n",
    " # Creation of dataframe from tweets list\n",
    " # Add or remove columns as you remove tweet information\n",
    " tweets_df = pd.DataFrame(tweets_list)\n",
    "except BaseException as e:\n",
    "  print('failed on_status,',str(e))\n",
    "  time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scraping specific search query\n",
    "text_query = '2020 US Election'\n",
    "count = 150\n",
    "try:\n",
    "    tweets = tweepy.Cursor(api.search_tweets, q=text_query).items(count)\n",
    " \n",
    " # Pulling information from tweets iterable object\n",
    "    tweets_list = [[tweet.created_at, tweet.id, tweet.text] for tweet in tweets]\n",
    " \n",
    " # Creation of dataframe from tweets list\n",
    " # Add or remove columns as you remove tweet information\n",
    "    tweets_df = pd.DataFrame(tweets_list)\n",
    " \n",
    "except BaseException as e:\n",
    "    print('failed on_status,',str(e))\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instaloader\n",
    "\n",
    "Install Command:\n",
    "- !pip3 install instaloader\n",
    "\n",
    "Description:\n",
    "- downloads public and private profiles, hashtags, user stories, feeds and saved media,\n",
    "- downloads comments, geotags and captions of each post,\n",
    "- automatically detects profile name changes and renames the target directory accordingly,\n",
    "- allows fine-grained customization of filters and where to store downloaded media,\n",
    "- automatically resumes previously-interrupted download iterations.\n",
    "\n",
    "instaloader [--comments] [--geotags]\n",
    "            [--stories] [--highlights] [--tagged] [--igtv]\n",
    "            [--login YOUR-USERNAME] [--fast-update]\n",
    "            profile | \"#hashtag\" | :stories | :feed | :saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting instaloader\n",
      "  Downloading instaloader-4.10.3.tar.gz (62 kB)\n",
      "     ---------------------------------------- 0.0/62.9 kB ? eta -:--:--\n",
      "     ------------ ------------------------- 20.5/62.9 kB 330.3 kB/s eta 0:00:01\n",
      "     ------------------------------ ------- 51.2/62.9 kB 525.1 kB/s eta 0:00:01\n",
      "     -------------------------------------- 62.9/62.9 kB 478.7 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests>=2.4 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from instaloader) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from requests>=2.4->instaloader) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from requests>=2.4->instaloader) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from requests>=2.4->instaloader) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from requests>=2.4->instaloader) (2023.11.17)\n",
      "Building wheels for collected packages: instaloader\n",
      "  Building wheel for instaloader (setup.py): started\n",
      "  Building wheel for instaloader (setup.py): finished with status 'done'\n",
      "  Created wheel for instaloader: filename=instaloader-4.10.3-py3-none-any.whl size=64800 sha256=e74d85ff83c71111b3aa668358596577c4687edea9aaea36f36e26d5b308fc88\n",
      "  Stored in directory: c:\\users\\kahhou_lau\\appdata\\local\\pip\\cache\\wheels\\79\\98\\b7\\4c15fe6680cf0e460b20fba742cf5052461e8b320f6f9f7e21\n",
      "Successfully built instaloader\n",
      "Installing collected packages: instaloader\n",
      "Successfully installed instaloader-4.10.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install instaloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['https://scontent-kul2-1.cdninstagram.com/v/t39.30808-6/413853325_18303547231130251_7778221327299527502_n.jpg?stp=dst-jpg_e15&_nc_ht=scontent-kul2-1.cdninstagram.com&_nc_cat=107&_nc_ohc=J8hzRc5wg0gAX8PCUHA&edm=AOQ1c0wAAAAA&ccb=7-5&ig_cache_key=MzI2NjAxMzE0MTAzNzEwOTc0NA%3D%3D.2-ccb7-5&oh=00_AfBSkMQoJ4vlUth3mpq9g1VA5nYzxQLUKOmvTR6YE3f0zg&oe=65A6E3EF&_nc_sid=8b3546', 'https://scontent-kul2-1.cdninstagram.com/v/t39.30808-6/394258941_18294093484130251_2272600211854056886_n.jpg?stp=dst-jpg_e35_p1080x1080_sh0.08&_nc_ht=scontent-kul2-1.cdninstagram.com&_nc_cat=107&_nc_ohc=AneGWvm0aGEAX_cs4iL&edm=AOQ1c0wAAAAA&ccb=7-5&ig_cache_key=MzIxODMwNTAwMTk2OTg3NzkwOA%3D%3D.2-ccb7-5&oh=00_AfCZ1dXYlm1RZe2cRn_RTAbimZLU2BBiZd0aX2Zj9_d4qw&oe=65A73A93&_nc_sid=8b3546'], 'https://scontent-kul2-1.cdninstagram.com/v/t51.2885-19/330996695_872055110760217_9044841881213205121_n.jpg?stp=dst-jpg_s320x320&_nc_ht=scontent-kul2-1.cdninstagram.com&_nc_cat=101&_nc_ohc=J7JYdllicOoAX-HyR9P&edm=AOQ1c0wBAAAA&ccb=7-5&oh=00_AfAFoV93FSktIfK77rOBTMBULB4odrit86ViA89Ua_7d_A&oe=65A61E86&_nc_sid=8b3546')\n"
     ]
    }
   ],
   "source": [
    "import instaloader\n",
    "import requests\n",
    "\n",
    "def get_instagram_pic_urls(username, max_count=2):\n",
    "    loader = instaloader.Instaloader()\n",
    "    profile = instaloader.Profile.from_username(loader.context, username)\n",
    "\n",
    "    image_name = \"ig_\" + username\n",
    "\n",
    "    image_urls = []\n",
    "\n",
    "    for i, post in enumerate(profile.get_posts()): \n",
    "        if i >= max_count:\n",
    "            break\n",
    "        image_url = post.url\n",
    "        image_urls.append(image_url)\n",
    "        # Optionally download the image in the same directory as the notebook\n",
    "        response = requests.get(image_url)\n",
    "        with open(image_name + str(i) + \".jpg\", \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "    \n",
    "    return image_urls, profile.profile_pic_url\n",
    "\n",
    "# Example usage\n",
    "print(get_instagram_pic_urls('brahderlau'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "L = instaloader.Instaloader()\n",
    "profile = instaloader.Profile.from_username(L.context, 'brahderlau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadCredentialsException",
     "evalue": "Login error: Wrong password.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadCredentialsException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m password \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBrahderlau99@\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43musername\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m instaloader\u001b[38;5;241m.\u001b[39mTwoFactorAuthRequiredException:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# The exception is raised if 2FA is required\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     verification_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter 2FA verification code: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Kahhou_Lau\\AppData\\Local\\anaconda3\\envs\\ads\\lib\\site-packages\\instaloader\\instaloader.py:651\u001b[0m, in \u001b[0;36mInstaloader.login\u001b[1;34m(self, user, passwd)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogin\u001b[39m(\u001b[38;5;28mself\u001b[39m, user: \u001b[38;5;28mstr\u001b[39m, passwd: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    644\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Log in to instagram with given username and password and internally store session object.\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \n\u001b[0;32m    646\u001b[0m \u001b[38;5;124;03m    :raises InvalidArgumentException: If the provided username does not exist.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;124;03m    :raises TwoFactorAuthRequiredException: First step of 2FA login done, now call\u001b[39;00m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;124;03m       :meth:`Instaloader.two_factor_login`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 651\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpasswd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kahhou_Lau\\AppData\\Local\\anaconda3\\envs\\ads\\lib\\site-packages\\instaloader\\instaloadercontext.py:306\u001b[0m, in \u001b[0;36mInstaloaderContext.login\u001b[1;34m(self, user, passwd)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resp_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthenticated\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resp_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;66;03m# '{\"authenticated\": false, \"user\": true, \"status\": \"ok\"}'\u001b[39;00m\n\u001b[1;32m--> 306\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m BadCredentialsException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogin error: Wrong password.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;66;03m# '{\"authenticated\": false, \"user\": false, \"status\": \"ok\"}'\u001b[39;00m\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;66;03m# Raise InvalidArgumentException rather than BadCredentialException, because BadCredentialException\u001b[39;00m\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;66;03m# triggers re-asking of password in Instaloader.interactive_login(), which makes no sense if the\u001b[39;00m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;66;03m# username is invalid.\u001b[39;00m\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidArgumentException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogin error: User \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(user))\n",
      "\u001b[1;31mBadCredentialsException\u001b[0m: Login error: Wrong password."
     ]
    }
   ],
   "source": [
    "username = 'brahderlau'\n",
    "password = ''\n",
    "try:\n",
    "    L.login(username, password)\n",
    "except instaloader.TwoFactorAuthRequiredException:\n",
    "    # The exception is raised if 2FA is required\n",
    "    verification_code = input(\"Enter 2FA verification code: \")\n",
    "    L.two_factor_login(verification_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-26 02:52:07\n",
      "brahderlau\\2023-12-26_02-52-07_UTC.jpg [Merry Christmas 🎅] json \n",
      "2023-10-21 07:04:34\n",
      "brahderlau\\2023-10-21_07-04-34_UTC.jpg [Unleash your style with Hermo…] json \n",
      "2023-09-30 09:06:41\n",
      "brahderlau\\2023-09-30_09-06-41_UTC.jpg [Have you rewarded yourself wi…] json \n",
      "2023-09-30 15:40:33\n",
      "brahderlau\\2023-09-30_15-40-33_UTC.jpg [Still eating MIXUE? Let’s che…] json \n",
      "2023-09-30 15:24:40\n",
      "brahderlau\\2023-09-30_15-24-40_UTC.jpg json \n",
      "2023-09-30 15:20:32\n",
      "brahderlau\\2023-09-30_15-20-32_UTC.jpg json \n",
      "2023-09-30 15:18:27\n",
      "brahderlau\\2023-09-30_15-18-27_UTC.jpg json \n",
      "2023-09-30 15:16:39\n",
      "brahderlau\\2023-09-30_15-16-39_UTC.jpg json \n",
      "2023-06-30 04:32:09\n",
      "brahderlau\\2023-06-30_04-32-09_UTC.jpg json \n",
      "2023-06-30 04:31:53\n",
      "brahderlau\\2023-06-30_04-31-53_UTC.jpg json \n",
      "2023-06-30 04:31:33\n",
      "brahderlau\\2023-06-30_04-31-33_UTC.jpg json \n",
      "2023-04-21 13:23:16\n",
      "brahderlau\\2023-04-21_13-23-16_UTC.jpg [Selamat Hari Raya Aidilfitri …] json \n",
      "2023-03-22 12:10:37\n",
      "brahderlau\\2023-03-22_12-10-37_UTC.jpg [Saya ingin mengambil kesempat…] json \n",
      "2023-02-18 00:45:02\n",
      "brahderlau\\2023-02-18_00-45-02_UTC.jpg [我还是够高的 😬] json \n",
      "2023-01-31 05:53:50\n",
      "brahderlau\\2023-01-31_05-53-50_UTC.jpg [Location📍: KSH Bahn Mi Yue Na…] brahderlau\\2023-01-31_05-53-50_UTC.mp4 json \n",
      "2023-01-23 14:45:29\n",
      "brahderlau\\2023-01-23_14-45-29_UTC.jpg [初二 : 不知不觉跟姐姐撞衫，就当作是我姐姐的一天出租男友…] json \n",
      "2023-01-23 14:36:46\n",
      "brahderlau\\2023-01-23_14-36-46_UTC.jpg [这样搭配 也兔hype了吧  #hypemalaysia …] json \n",
      "2023-01-23 09:29:42\n",
      "brahderlau\\2023-01-23_09-29-42_UTC.jpg [两只老兔🐰🐰，一起兔two兔🤩  #hypemalaysi…] brahderlau\\2023-01-23_09-29-42_UTC.mp4 json \n",
      "2023-01-22 07:52:08\n",
      "brahderlau\\2023-01-22_07-52-08_UTC.jpg [不一样颜色的服装 意味着我们互相弥补自己的缺点 这样才是一…] json \n",
      "2023-01-22 07:02:29\n",
      "brahderlau\\2023-01-22_07-02-29_UTC.jpg [2023胖兔兔的我🐰] json \n",
      "2023-01-22 04:15:53\n",
      "brahderlau\\2023-01-22_04-15-53_UTC.jpg [Happy Chinese New Year of Rab…] json \n",
      "2023-01-21 16:50:41\n",
      "brahderlau\\2023-01-21_16-50-41_UTC_1.jpg brahderlau\\2023-01-21_16-50-41_UTC_2.jpg brahderlau\\2023-01-21_16-50-41_UTC_3.jpg [除夕 - 简单又美味的佳肴 回味无穷 也祝爷爷八十四大寿 …] json \n",
      "2023-01-21 15:54:12\n",
      "brahderlau\\2023-01-21_15-54-12_UTC.jpg [除夕前一夜, 一年一度的老同学聚会🏫😃] json \n",
      "2023-01-21 15:45:03\n",
      "brahderlau\\2023-01-21_15-45-03_UTC.jpg [Why do they have so many hate…] json \n",
      "2023-01-20 02:48:19\n",
      "brahderlau\\2023-01-20_02-48-19_UTC.jpg [Is it logical to buy a house …] json \n",
      "2023-01-18 14:12:07\n",
      "brahderlau\\2023-01-18_14-12-07_UTC_1.jpg brahderlau\\2023-01-18_14-12-07_UTC_2.jpg [[ 𝐇𝐨𝐰 𝐭𝐨 𝐫𝐞𝐝𝐮𝐜𝐞 𝐃𝐄𝐁𝐓 ⁉️ ]  Th…] json \n",
      "2023-01-17 14:42:02\n",
      "brahderlau\\2023-01-17_14-42-02_UTC_1.jpg brahderlau\\2023-01-17_14-42-02_UTC_2.jpg [[ 𝐇𝐨𝐰 𝐭𝐨 𝐢𝐧𝐜𝐫𝐞𝐚𝐬𝐞 𝐬𝐚𝐯𝐢𝐧𝐠𝐬 𝐛𝐲 …] json \n",
      "2023-01-16 11:24:18\n",
      "brahderlau\\2023-01-16_11-24-18_UTC_1.jpg brahderlau\\2023-01-16_11-24-18_UTC_2.jpg brahderlau\\2023-01-16_11-24-18_UTC_3.jpg [[𝐄𝐱𝐩𝐞𝐫𝐢𝐞𝐧𝐜𝐞 𝐉𝐨𝐢𝐧𝐢𝐧𝐠 𝐒𝐮𝐧𝐝𝐚𝐲 𝐒𝐞…] json \n",
      "2023-01-15 07:29:43\n",
      "brahderlau\\2023-01-15_07-29-43_UTC.jpg [𝐔𝐏𝐃𝐀𝐓𝐄 𝟑: 𝐅𝐞𝐞𝐝𝐛𝐚𝐜𝐤 𝐟𝐫𝐨𝐦 𝐨𝐭𝐡𝐞𝐫…] json \n",
      "2023-01-14 15:47:11\n",
      "brahderlau\\2023-01-14_15-47-11_UTC_1.jpg brahderlau\\2023-01-14_15-47-11_UTC_2.jpg brahderlau\\2023-01-14_15-47-11_UTC_3.jpg brahderlau\\2023-01-14_15-47-11_UTC_4.jpg brahderlau\\2023-01-14_15-47-11_UTC_5.jpg brahderlau\\2023-01-14_15-47-11_UTC_6.jpg brahderlau\\2023-01-14_15-47-11_UTC_7.jpg brahderlau\\2023-01-14_15-47-11_UTC_8.jpg [𝐔𝐏𝐃𝐀𝐓𝐄 𝟐: 𝐄𝐱𝐩𝐞𝐫𝐢𝐞𝐧𝐜𝐞 𝐣𝐨𝐢𝐧𝐢𝐧𝐠 …] json \n",
      "2023-01-14 09:17:43\n",
      "brahderlau\\2023-01-14_09-17-43_UTC_1.jpg brahderlau\\2023-01-14_09-17-43_UTC_2.jpg brahderlau\\2023-01-14_09-17-43_UTC_3.jpg brahderlau\\2023-01-14_09-17-43_UTC_4.jpg brahderlau\\2023-01-14_09-17-43_UTC_5.jpg brahderlau\\2023-01-14_09-17-43_UTC_6.jpg brahderlau\\2023-01-14_09-17-43_UTC_7.jpg brahderlau\\2023-01-14_09-17-43_UTC_8.jpg [[𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧 𝐨𝐟 𝐭𝐡𝐞 𝐝𝐚𝐲: 𝐅𝐀𝐑 𝐂𝐚𝐩…] json \n",
      "2023-01-01 07:38:34\n",
      "brahderlau\\2023-01-01_07-38-34_UTC.jpg [Thank you everyone being part…] brahderlau\\2023-01-01_07-38-34_UTC.mp4 json \n",
      "2022-12-25 15:56:02\n",
      "brahderlau\\2022-12-25_15-56-02_UTC.jpg [Let’s celebrate Merry Christm…] json \n",
      "2022-12-25 15:17:50\n",
      "brahderlau\\2022-12-25_15-17-50_UTC.jpg [Santa🎅when looking for the ba…] json \n",
      "2022-12-25 14:59:19\n",
      "brahderlau\\2022-12-25_14-59-19_UTC.jpg [Santa Claus robbing the kids …] json \n",
      "2022-12-25 14:36:11\n",
      "brahderlau\\2022-12-25_14-36-11_UTC.jpg [HO HO HO 🎅❤️] brahderlau\\2022-12-25_14-36-11_UTC.mp4 json \n",
      "2022-12-13 12:38:28\n",
      "brahderlau\\2022-12-13_12-38-28_UTC.jpg [Follow trend 😬 malu tapi mahu…] brahderlau\\2022-12-13_12-38-28_UTC.mp4 json \n",
      "2022-12-05 15:07:36\n",
      "brahderlau\\2022-12-05_15-07-36_UTC.jpg [都不懂在指什么，看什么🤔] json \n",
      "2022-12-04 14:02:14\n",
      "brahderlau\\2022-12-04_14-02-14_UTC.jpg brahderlau\\2022-12-04_14-02-14_UTC.mp4 json \n",
      "2022-12-03 04:38:48\n",
      "brahderlau\\2022-12-03_04-38-48_UTC.jpg [🎓👉🏻☁️] json \n",
      "2022-12-01 15:25:42\n",
      "brahderlau\\2022-12-01_15-25-42_UTC.jpg [Let me go 🎓✊🏻] json \n",
      "2022-11-30 05:56:32\n",
      "brahderlau\\2022-11-30_05-56-32_UTC.jpg [Flying🎓☁️] json \n",
      "2022-11-29 15:11:59\n",
      "brahderlau\\2022-11-29_15-11-59_UTC.jpg [🎓🌸] json \n",
      "2022-11-28 15:26:44\n",
      "brahderlau\\2022-11-28_15-26-44_UTC.jpg [Pool-walking at Manatidur 😇] brahderlau\\2022-11-28_15-26-44_UTC.mp4 json \n",
      "2022-11-19 13:41:39\n",
      "brahderlau\\2022-11-19_13-41-39_UTC.jpg [Still waiting the result for …] json \n",
      "2022-11-19 08:39:47\n",
      "brahderlau\\2022-11-19_08-39-47_UTC.jpg [Saya sudah menunaikan kewajip…] json \n",
      "2022-11-19 08:22:14\n",
      "brahderlau\\2022-11-19_08-22-14_UTC.jpg [Setiap undian daripada anda a…] json \n",
      "2022-11-19 07:55:17\n",
      "brahderlau\\2022-11-19_07-55-17_UTC.jpg [PRU15 🇲🇾✅   Saya sudah undi, …] json \n",
      "2022-06-19 16:19:41\n",
      "brahderlau\\2022-06-19_16-19-41_UTC.jpg [Get tanned while sleeping on …] json \n",
      "2022-06-18 12:31:06\n",
      "brahderlau\\2022-06-18_12-31-06_UTC.jpg [Trying to blend in with the c…] json \n",
      "2022-05-01 15:49:30\n",
      "brahderlau\\2022-05-01_15-49-30_UTC.jpg [Selamat Hari Raya Aidilfitri,…] json \n",
      "2022-03-13 04:27:17\n",
      "brahderlau\\2022-03-13_04-27-17_UTC.jpg [Looks like broccolis from far 😬] json \n",
      "2022-02-05 00:57:34\n",
      "brahderlau\\2022-02-05_00-57-34_UTC.jpg [TQ for the amazing digital ar…] json \n",
      "2022-02-04 08:22:26\n",
      "brahderlau\\2022-02-04_08-22-26_UTC.jpg [多善用身边的物体去弥补自己的缺点🙂] json \n",
      "2022-02-03 04:08:12\n",
      "brahderlau\\2022-02-03_04-08-12_UTC.jpg [树叶抚摸我的脸颊 凉快无比 #初二回娘家❤️] json \n",
      "2021-12-31 13:58:40\n",
      "brahderlau\\2021-12-31_13-58-40_UTC.jpg [Last photo of 2021] json \n",
      "2021-11-05 07:27:28\n",
      "brahderlau\\2021-11-05_07-27-28_UTC.jpg [品一品茶的香味] brahderlau\\2021-11-05_07-27-28_UTC.mp4 json \n",
      "2021-10-12 17:38:30\n",
      "brahderlau\\2021-10-12_17-38-30_UTC_1.jpg brahderlau\\2021-10-12_17-38-30_UTC_2.jpg brahderlau\\2021-10-12_17-38-30_UTC_3.jpg [Happy 22nd Birthday to me 🐻 T…] json \n",
      "2021-10-04 11:53:17\n",
      "brahderlau\\2021-10-04_11-53-17_UTC.jpg brahderlau\\2021-10-04_11-53-17_UTC.mp4 json \n",
      "2021-08-14 14:22:56\n",
      "brahderlau\\2021-08-14_14-22-56_UTC.jpg [Love the filter :3] brahderlau\\2021-08-14_14-22-56_UTC.mp4 json \n",
      "2021-08-14 13:38:20\n",
      "brahderlau\\2021-08-14_13-38-20_UTC.jpg [嘿嘿] brahderlau\\2021-08-14_13-38-20_UTC.mp4 json \n",
      "2021-08-02 05:04:06\n",
      "brahderlau\\2021-08-02_05-04-06_UTC.jpg [2nd Dose Completed ✅] json \n",
      "2021-07-12 07:49:13\n",
      "brahderlau\\2021-07-12_07-49-13_UTC.jpg [I am vaccinated!!!!!!!! NEXT …] json \n",
      "2021-02-11 13:55:54\n",
      "brahderlau\\2021-02-11_13-55-54_UTC.jpg [新年尝试新发型 #除夕快乐 #牛年大吉 #牛转乾坤] json \n",
      "2020-10-12 15:59:27\n",
      "brahderlau\\2020-10-12_15-59-27_UTC_1.jpg brahderlau\\2020-10-12_15-59-27_UTC_2.jpg brahderlau\\2020-10-12_15-59-27_UTC_3.jpg brahderlau\\2020-10-12_15-59-27_UTC_4.jpg brahderlau\\2020-10-12_15-59-27_UTC_5.jpg brahderlau\\2020-10-12_15-59-27_UTC_6.jpg [Thank you all of you for addi…] json \n",
      "2020-01-28 09:18:32\n",
      "brahderlau\\2020-01-28_09-18-32_UTC.jpg [别跟我虚伪，我懒得敷衍！ #chorsam #ootd #初三] json \n",
      "2020-01-26 07:36:48\n",
      "brahderlau\\2020-01-26_07-36-48_UTC.jpg [新的一年，除了祝大家暴富也可以试试抱我 🙈#choryee…] json \n",
      "2020-01-25 04:36:46\n",
      "brahderlau\\2020-01-25_04-36-46_UTC.jpg [新年快乐，我对自己说，希望你眼里长着太阳，笑里全是坦荡。#…] json \n",
      "2020-01-18 13:18:57\n",
      "brahderlau\\2020-01-18_13-18-57_UTC.jpg [Okay kayo] json \n",
      "2019-12-10 03:35:44\n",
      "brahderlau\\2019-12-10_03-35-44_UTC_1.jpg brahderlau\\2019-12-10_03-35-44_UTC_2.jpg brahderlau\\2019-12-10_03-35-44_UTC_3.jpg [Terbaek Geng QuQu @ Vloft Gam…] json \n",
      "2019-12-10 02:59:02\n",
      "brahderlau\\2019-12-10_02-59-02_UTC.jpg [When was the last time you vi…] json \n",
      "2019-09-04 12:50:37\n",
      "brahderlau\\2019-09-04_12-50-37_UTC.jpg [Don't study me. You won't gra…] json \n",
      "2019-08-31 04:27:50\n",
      "brahderlau\\2019-08-31_04-27-50_UTC.jpg ['Sayangi Malaysiaku: Malaysia…] json \n",
      "2018-11-17 18:17:50\n",
      "brahderlau\\2018-11-17_18-17-50_UTC.jpg [If you can't find the light i…] json \n",
      "2018-10-14 14:19:37\n",
      "brahderlau\\2018-10-14_14-19-37_UTC.jpg [😂😂😂] json \n",
      "2018-10-13 14:15:14\n",
      "brahderlau\\2018-10-13_14-15-14_UTC.jpg [Thank You Everyone Who Made M…] json \n",
      "2018-10-13 14:11:28\n",
      "brahderlau\\2018-10-13_14-11-28_UTC.jpg json \n",
      "2018-10-13 14:06:47\n",
      "brahderlau\\2018-10-13_14-06-47_UTC.jpg json \n",
      "2018-05-19 11:13:47\n",
      "brahderlau\\2018-05-19_11-13-47_UTC.jpg json \n",
      "2018-05-13 07:51:18\n",
      "brahderlau\\2018-05-13_07-51-18_UTC.jpg [#pondview #peace #brahderphot…] json \n",
      "2018-05-12 12:32:37\n",
      "brahderlau\\2018-05-12_12-32-37_UTC.jpg [Happy Mother's Day to all the…] json \n",
      "2018-05-11 11:46:59\n",
      "brahderlau\\2018-05-11_11-46-59_UTC.jpg [#oillamp #brahderphotography …] json \n",
      "2018-04-25 15:55:05\n",
      "brahderlau\\2018-04-25_15-55-05_UTC.jpg [Do what is right, not what is…] json \n",
      "2018-04-07 09:11:54\n",
      "brahderlau\\2018-04-07_09-11-54_UTC.jpg [Life mission accomplished!!! …] json \n",
      "2017-11-25 10:00:46\n",
      "brahderlau\\2017-11-25_10-00-46_UTC.jpg [I am not for sale, i deserve …] json \n",
      "2017-11-08 12:48:13\n",
      "brahderlau\\2017-11-08_12-48-13_UTC.jpg [When the lecturer is absent b…] json \n",
      "2017-10-24 12:04:01\n",
      "brahderlau\\2017-10-24_12-04-01_UTC.jpg [这首歌其实就是给我整体上一种很平静，温暖，舒服的感觉。他让…] brahderlau\\2017-10-24_12-04-01_UTC.mp4 json \n",
      "2017-10-07 12:13:34\n",
      "brahderlau\\2017-10-07_12-13-34_UTC.jpg [Never loves anyone who treats…] json \n",
      "2017-10-05 10:15:04\n",
      "brahderlau\\2017-10-05_10-15-04_UTC.jpg [It's not how much we have, bu…] json \n",
      "2017-09-10 08:15:22\n",
      "brahderlau\\2017-09-10_08-15-22_UTC_1.jpg brahderlau\\2017-09-10_08-15-22_UTC_2.jpg [Love is like stock trading, y…] json \n",
      "2017-06-25 09:30:25\n",
      "brahderlau\\2017-06-25_09-30-25_UTC.jpg json \n",
      "2017-06-19 03:01:06\n",
      "brahderlau\\2017-06-19_03-01-06_UTC.jpg json \n",
      "2017-06-18 12:35:38\n",
      "brahderlau\\2017-06-18_12-35-38_UTC.jpg [Sunset of the day :)] json \n",
      "2017-06-13 10:37:31\n",
      "brahderlau\\2017-06-13_10-37-31_UTC.jpg [Hiiii Mum! Love u always :)] json \n",
      "2017-06-12 02:02:52\n",
      "brahderlau\\2017-06-12_02-02-52_UTC.jpg [Light from the Heaven! #Blessed] json \n",
      "2017-06-10 12:02:09\n",
      "brahderlau\\2017-06-10_12-02-09_UTC.jpg [#NoFilter #Natural #Mysteriou…] json \n",
      "2017-04-30 14:14:08\n",
      "brahderlau\\2017-04-30_14-14-08_UTC.jpg [Always look further. #candid] json \n",
      "2017-04-20 04:46:24\n",
      "brahderlau\\2017-04-20_04-46-24_UTC.jpg [#drumstickawesome #iamawesome] brahderlau\\2017-04-20_04-46-24_UTC.mp4 json \n",
      "2017-03-24 06:38:06\n",
      "brahderlau\\2017-03-24_06-38-06_UTC.jpg [Thank you eGG!!!!! Love ya #e…] brahderlau\\2017-03-24_06-38-06_UTC.mp4 json \n",
      "2017-03-23 09:07:41\n",
      "brahderlau\\2017-03-23_09-07-41_UTC.jpg [Hiiiii pindapanda :3 #360toge…] json \n",
      "2017-03-01 12:33:50\n",
      "brahderlau\\2017-03-01_12-33-50_UTC.jpg [Sea view :)] json \n",
      "2017-02-28 04:48:16\n",
      "brahderlau\\2017-02-28_04-48-16_UTC.jpg [Hi Guys! This is my first new…] brahderlau\\2017-02-28_04-48-16_UTC.mp4 json \n",
      "2017-01-28 06:44:52\n",
      "brahderlau\\2017-01-28_06-44-52_UTC.jpg [#choryat] json \n",
      "2016-11-30 03:14:30\n",
      "brahderlau\\2016-11-30_03-14-30_UTC.jpg [Last day wearing school unifo…] json \n",
      "2016-10-24 12:08:58\n",
      "brahderlau\\2016-10-24_12-08-58_UTC.jpg [#me I choose bear bear :3 #br…] json \n",
      "2016-10-16 13:02:10\n",
      "brahderlau\\2016-10-16_13-02-10_UTC.jpg [Fate brought us together , we…] json \n"
     ]
    }
   ],
   "source": [
    "# Download Posts in a Specific Period\n",
    "from datetime import datetime\n",
    "from itertools import dropwhile, takewhile\n",
    "\n",
    "import instaloader\n",
    "\n",
    "L = instaloader.Instaloader()\n",
    "\n",
    "posts = instaloader.Profile.from_username(L.context, \"brahderlau\").get_posts()\n",
    "\n",
    "SINCE = datetime(2024, 1, 1)\n",
    "UNTIL = datetime(2024, 1, 12)\n",
    "\n",
    "for post in takewhile(lambda p: p.date <= UNTIL, dropwhile(lambda p: p.date > SINCE, posts)):\n",
    "    print(post.date)\n",
    "    L.download_post(post, \"brahderlau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching likes of all posts of profile realdonaldtrump.\n",
      "<Post C2BRI_QsPyp>\n"
     ]
    },
    {
     "ename": "LoginRequiredException",
     "evalue": "--login required to access likes of a post.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLoginRequiredException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m post \u001b[38;5;129;01min\u001b[39;00m profile\u001b[38;5;241m.\u001b[39mget_posts():\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(post)\n\u001b[1;32m---> 16\u001b[0m     likes \u001b[38;5;241m=\u001b[39m likes \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_likes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# | means union\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching followers of profile \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(profile\u001b[38;5;241m.\u001b[39musername))\n\u001b[0;32m     19\u001b[0m followers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(profile\u001b[38;5;241m.\u001b[39mget_followers())\n",
      "File \u001b[1;32mc:\\Users\\Kahhou_Lau\\AppData\\Local\\anaconda3\\envs\\ads\\lib\\site-packages\\instaloader\\structures.py:685\u001b[0m, in \u001b[0;36mPost.get_likes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;124;03mIterate over all likes of the post. A :class:`Profile` instance of each likee is yielded.\u001b[39;00m\n\u001b[0;32m    680\u001b[0m \n\u001b[0;32m    681\u001b[0m \u001b[38;5;124;03m.. versionchanged:: 4.5.4\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;124;03m   Require being logged in (as required by Instagram).\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mis_logged_in:\n\u001b[1;32m--> 685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LoginRequiredException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--login required to access likes of a post.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;66;03m# Avoid doing additional requests if there are no comments\u001b[39;00m\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mLoginRequiredException\u001b[0m: --login required to access likes of a post."
     ]
    }
   ],
   "source": [
    "# Get likes of a profile / Ghost followers (Fake Accounts)\n",
    "import instaloader\n",
    "\n",
    "L = instaloader.Instaloader()\n",
    "\n",
    "USER = \"realdonaldtrump\"\n",
    "PROFILE = USER\n",
    "\n",
    "\n",
    "profile = instaloader.Profile.from_username(L.context, PROFILE)\n",
    "\n",
    "likes = set()\n",
    "print(\"Fetching likes of all posts of profile {}.\".format(profile.username))\n",
    "for post in profile.get_posts():\n",
    "    print(post)\n",
    "    likes = likes | set(post.get_likes()) # | means union\n",
    "\n",
    "print(\"Fetching followers of profile {}.\".format(profile.username))\n",
    "followers = set(profile.get_followers())\n",
    "\n",
    "ghosts = followers - likes\n",
    "\n",
    "print(\"Storing ghosts into file.\")\n",
    "with open(\"inactive-users.txt\", 'w') as f:\n",
    "    for ghost in ghosts:\n",
    "        print(ghost.username, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Getting Top X Posts of a user (most liked)\n",
    "\n",
    "# from itertools import islice\n",
    "# from math import ceil\n",
    "\n",
    "# from instaloader import Instaloader, Profile\n",
    "\n",
    "# PROFILE = 'jyjosephine'        # profile to download from\n",
    "# X_percentage = 10    # percentage of posts that should be downloaded\n",
    "\n",
    "# L = Instaloader()\n",
    "\n",
    "# profile = Profile.from_username(L.context, PROFILE)\n",
    "# posts_sorted_by_likes = sorted(profile.get_posts(),\n",
    "#                                key=lambda p: p.likes + p.comments,\n",
    "#                                reverse=True)\n",
    "\n",
    "\n",
    "# for post in islice(posts_sorted_by_likes, ceil(profile.mediacount * X_percentage / 100)):\n",
    "#     L.download_post(post, PROFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: PRAW\n",
    "\n",
    "Install Command:\n",
    "- pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id='mTF5k367VUFFmQ', client_secret='mjhD5xlOjtzDM8uovN9mzkhVOC2KbA', user_agent='User-Agent: Mozilla/5.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # get 10 hot posts from the MachineLearning subreddit\n",
    "# hot_posts = reddit.subreddit('Coronavirus').hot(limit=10)\n",
    "# for post in hot_posts:\n",
    "#     print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # get hottest posts from all subreddits\n",
    "# hot_posts = reddit.subreddit('all').hot(limit=10)\n",
    "# for post in hot_posts:\n",
    "#     print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# posts = []\n",
    "# ml_subreddit = reddit.subreddit('Coronavirus')\n",
    "# for post in ml_subreddit.hot(limit=10):\n",
    "#     posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "# posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # get Coronavirus subreddit data\n",
    "# ml_subreddit = reddit.subreddit('Coronavirus')\n",
    "\n",
    "# print(ml_subreddit.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = reddit.submission(url=\"https://www.reddit.com/r/MapPorn/comments/a3p0uq/an_image_of_gps_tracking_of_multiple_wolves_in/\")\n",
    "# or \n",
    "# submission = reddit.submission(id=\"a3p0uq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for top_level_comment in submission.comments:\n",
    "#     print(top_level_comment.body)\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from praw.models import MoreComments\n",
    "\n",
    "# for top_level_comment in submission.comments:\n",
    "#     if isinstance(top_level_comment, MoreComments):\n",
    "#         continue\n",
    "#     print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=0)\n",
    "for top_level_comment in submission.comments:\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=0)\n",
    "for comment in submission.comments.list():\n",
    "    print(comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRON JOB\n",
    "\n",
    "What is CRON?\n",
    "\n",
    "The system utility cron can be used to schedule programs to run automatically at predetermined intervals.\n",
    "\n",
    "Cron is a standard Unix utility that is used to schedule commands for automatic execution at specific intervals. For instance, you might have a script that produces web statistics that you want to run once a day automatically at 5:00 AM.\n",
    "\n",
    "Commands involving cron are referred to as \"cron jobs.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo we gonna open a Python based GUI that displays \"Hello World\"\n",
    "- pip install tk\n",
    "- Create a python file called \"hello_world.py\"\n",
    "- Remember the path you save it at \"E:/hello_world.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tk\n",
      "  Downloading tk-0.1.0-py3-none-any.whl (3.9 kB)\n",
      "Installing collected packages: tk\n",
      "Successfully installed tk-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tkinter as tk \n",
    "\n",
    "root= tk.Tk() \n",
    " \n",
    "canvas1 = tk.Canvas(root, width = 300, height = 300)\n",
    "canvas1.pack()\n",
    "\n",
    "label1 = tk.Label(root, text='Hello World!')\n",
    "canvas1.create_window(150, 150, window=label1)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For MacOS/Ubuntu/Linux-based systems (Usually in production)\n",
    "\n",
    "To open CRON, simply use the following commands\n",
    "```\n",
    "crontab -e\n",
    "```\n",
    "To check status for any errors, stop or start CRON service\n",
    "```\n",
    "service cron status\n",
    "service cron stop\n",
    "service cron start\n",
    "```\n",
    "\n",
    "The following is the command you want to run in CRON\n",
    "\n",
    "```\n",
    "30 3 * * * python /usr/home/hello_world.py\n",
    "```\n",
    "\n",
    "To learn more about the timing notation, here is a fantastic resource:\n",
    "https://crontab.guru/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Windows (Extra Knowledge)\n",
    "\n",
    "Step-1: Prepare the Python Script\n",
    "\n",
    "Step-2: Create Batch File to Run the Python Script\n",
    "\n",
    "- \"Path where your Python exe is stored\\python.exe\" \"Path where your Python script is stored\\script name.py\"\n",
    "\n",
    "```bat\n",
    "\"C:\\\\Users\\\\daffy\\\\Anaconda3\\\\python.exe\" \"E:\\\\hello_world.py\"\n",
    "pause\n",
    "```\n",
    "- Name the file 'run_python.bat'\n",
    "\n",
    "Step-3: Schedule the Python Script using Windows Scheduler\n",
    "- First, open the Control Panel and then click on the Administrative Tools:\n",
    "- Next, double-click on the Task Scheduler, and then choose the option to ‘Create Basic Task…’\n",
    "- Type a name for your task (you can also type a description if needed), and then press Next.\n",
    "- Next, I chose to start the task ‘Daily’ since we wish to run the Python script daily at 6am:\n",
    "- The action will then recur everyday at 6am, staring from 2019-04-01. You can adjust those timing parameters to suit your needs.\n",
    "- Select, Start a program, and then press Next:\n",
    "- Next, use the Browse button to find the batch file that runs the Python script. In my case, I placed the Run_Python_Script batch file on my Desktop:\n",
    "- Finally, click on Finish, and you should be good to go:\n",
    "- From this point onward, you’ll be greeted with ‘Hello World!’ everyday at 6am:\n",
    "- Please note that there is more than one way to execute Python Script using the Windows Scheduler. I chose to create a batch file to run the Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
