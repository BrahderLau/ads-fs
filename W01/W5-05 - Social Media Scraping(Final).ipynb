{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W5-05 - Social Media Scraping\n",
    "\n",
    "## Exercise(s): Learn how to perform Data Scraping\n",
    "\n",
    "\n",
    "Objective: Learn to scrape Facebook, Instagram, Twitters, Reddit (FITR)\n",
    "\n",
    "Competencies:\n",
    "-\tParticipants will be able to scrape Facebook, Instagram, Twitters, Reddit (FITR)\n",
    "\n",
    "Tools: Python, Anaconda, Jupyter\n",
    "\n",
    "Analysis case study: -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra note for yesterday\n",
    "\n",
    "What is a User Agent?\n",
    "In computing, a user agent is anything that is acting on behalf of the user. A Borwser's User agent is a string or line of text, containing information about the browser and operating system, which is sent to the web server in the HTTP request header. This allows the website to customize the content, to best suit the capabilities of the particular device, but sharing this information also raises certain privacy issues.\n",
    "\n",
    "In a typical HTTP request/response cycle you can view the browser's User-Agent inside the HTTP request/response header. It looks something like:\n",
    "\n",
    "![changing-user-agent-1](changing-user-agent-1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook_scraper\n",
    "\n",
    "Install Command:\n",
    "- pip install facebook-scraper\n",
    "\n",
    "Description:\n",
    "- Allows you to scrape public facebook pages without an API Key\n",
    "\n",
    "Optional parameters:\n",
    "- group: group id, to scrape groupHi s instead of pages. Default is None.\n",
    "- pages: how many pages of posts to request, usually the first page has 2 posts and the rest 4. Default is 10.\n",
    "- timeout: how many seconds to wait before timing out. Default is 5.\n",
    "- credentials: tuple of user and password to login before requesting the posts. Default is None.\n",
    "- extra_info: bool, if true the function will try to do an extra request to get the post reactions. Default is False.\n",
    "- youtube_dl: bool, use Youtube-DL for (high-quality) video extraction. You need to have youtube-dl installed on your environment. Default is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from facebook_scraper import get_posts\n",
    "\n",
    "for post in get_posts('nintendo', pages=1):\n",
    "    print(post['text'][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample data returned\n",
    "\n",
    "{'post_id': '2257188721032235',\n",
    " 'text': 'Donâ€™t let this diminutive version of the Hero of Time fool you, '\n",
    "         'Young Link is just as heroic as his fully grown version! Young Link '\n",
    "         'joins the Super Smash Bros. series of amiibo figures!',\n",
    " 'time': datetime.datetime(2019, 4, 29, 12, 0, 1),\n",
    " 'image': 'https://scontent.flim16-1.fna.fbcdn.net'\n",
    "          '/v/t1.0-0/cp0/e15/q65/p320x320'\n",
    "          '/58680860_2257182054366235_1985558733786185728_n.jpg'\n",
    "          '?_nc_cat=1&_nc_ht=scontent.flim16-1.fna'\n",
    "          '&oh=31b0ba32ec7886e95a5478c479ba1d38&oe=5D6CDEE4',\n",
    " 'images': ['https://scontent.flim16-1.fna.fbcdn.net'\n",
    "          '/v/t1.0-0/cp0/e15/q65/p320x320'\n",
    "          '/58680860_2257182054366235_1985558733786185728_n.jpg'\n",
    "          '?_nc_cat=1&_nc_ht=scontent.flim16-1.fna'\n",
    "          '&oh=31b0ba32ec7886e95a5478c479ba1d38&oe=5D6CDEE4'],\n",
    " 'likes': 2036,\n",
    " 'comments': 214,\n",
    " 'shares': 0,\n",
    " 'reactions': {'like': 135, 'love': 64, 'haha': 10, 'wow': 4, 'anger': 1},  # if `extra_info` was set\n",
    " 'post_url': 'https://m.facebook.com/story.php'\n",
    "             '?story_fbid=2257188721032235&id=119240841493711',\n",
    " 'link': 'https://bit.ly/something'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Download comments for a public Facebook post.\n",
    "# \"\"\"\n",
    "\n",
    "# import facebook_scraper as fs\n",
    "\n",
    "# # get POST_ID from the URL of the post which can have the following structure:\n",
    "# # https://www.facebook.com/USER/posts/POST_ID\n",
    "# # https://www.facebook.com/groups/GROUP_ID/posts/POST_ID\n",
    "# POST_ID = \"pfbid02NsuAiBU9o1ouwBrw1vYAQ7khcVXvz8F8zMvkVat9UJ6uiwdgojgddQRLpXcVBqYbl\"\n",
    "\n",
    "# # number of comments to download\n",
    "# MAX_COMMENTS = 2\n",
    "\n",
    "# # get the post (this gives a generator)\n",
    "# gen = fs.get_posts(\n",
    "#     post_urls=[POST_ID],\n",
    "#     options={\"comments\": MAX_COMMENTS, \"progress\": True}\n",
    "# )\n",
    "\n",
    "# # take 1st element of the generator which is the post we requested\n",
    "# post = next(gen)\n",
    "\n",
    "# # extract the comments part\n",
    "# comments = post['comments_full']\n",
    "\n",
    "# # process comments as you want...\n",
    "# for comment in comments:\n",
    "\n",
    "#     # e.g. ...print them\n",
    "#     print(comment)\n",
    "\n",
    "#     # e.g. ...get the replies for them\n",
    "#     for reply in comment['replies']:\n",
    "#         print(' ', reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from facebook_scraper import get_profile\n",
    "# get_profile(\"zuck\") # Or get_profile(\"zuck\", cookies=\"cookies.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for post in get_posts('Nintendo', pages=10):\n",
    "    print(post['text'][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facebook_scraper import get_posts\n",
    "\n",
    "group_id = '797518550594060' #DeveloperKaki\n",
    "username = 'khlau_1012@hotmail.com'\n",
    "password = 'your_password'\n",
    "\n",
    "for post in get_posts(group=group_id, pages=1, credentials=(username , password)):\n",
    "    print(post['text'][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Friend_count': None,\n",
       " 'Follower_count': None,\n",
       " 'Following_count': None,\n",
       " 'cover_photo': 'https://scontent-kul2-1.xx.fbcdn.net/v/t31.18172-8/19575079_10103832396388711_8894816584589808440_o.jpg?stp=cp0_dst-jpg_e15_fr_q65&_nc_cat=109&ccb=1-7&_nc_sid=d668b3&_nc_ohc=J-cM3PlmuMgAX_USb2F&_nc_ht=scontent-kul2-1.xx&oh=00_AfB4iAoBhDPdfFSTYNa4fygGyamPVJfVVjr_-nO2QzW9Pw&oe=65C9727D',\n",
       " 'profile_picture': 'https://scontent-kul2-1.xx.fbcdn.net/v/t39.30808-1/312257846_10114737758665291_6588360857015169674_n.jpg?stp=cp0_dst-jpg_e15_q65_s120x120&_nc_cat=1&ccb=1-7&_nc_sid=4da83f&_nc_ohc=P9djs7GevxAAX8qSBNc&_nc_ht=scontent-kul2-1.xx&oh=00_AfDu4htlN-5cpvpWw2-K2e9LEhAaDwKEsAujmPR8TnE93w&oe=65A6541E',\n",
       " 'id': '4',\n",
       " 'Name': 'Mark Zuckerberg',\n",
       " 'à¸—à¸µà¹ˆà¸—à¸³à¸‡à¸²à¸™': 'Chan Zuckerberg Initiative\\n1 à¸˜à¸±à¸™à¸§à¸²à¸„à¸¡ 2015 - à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™\\nMeta\\nFounder and CEO\\n4 à¸à¸¸à¸¡à¸ à¸²à¸à¸±à¸™à¸˜à¹Œ 2004 - à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™\\nPalo Alto, California\\nBringing the world closer together.',\n",
       " 'à¸à¸²à¸£à¸¨à¸¶à¸à¸©à¸²': 'Harvard University\\nComputer Science and Psychology\\n30 à¸ªà¸´à¸‡à¸«à¸²à¸„à¸¡ 2002 - 30 à¹€à¸¡à¸©à¸²à¸¢à¸™ 2004\\nPhillips Exeter Academy\\nClassics\\nà¸ˆà¸šà¸à¸²à¸£à¸¨à¸¶à¸à¸©à¸²à¹€à¸¡à¸·à¹ˆà¸­à¸›à¸µ 2002\\nArdsley High School\\nà¹‚à¸£à¸‡à¹€à¸£à¸µà¸¢à¸™à¸¡à¸±à¸˜à¸¢à¸¡\\nà¸ªà¸´à¸‡à¸«à¸²à¸„à¸¡ 1998 - à¸à¸¤à¸©à¸ à¸²à¸„à¸¡ 2000',\n",
       " 'à¸ªà¸–à¸²à¸™à¸—à¸µà¹ˆà¸—à¸µà¹ˆà¹€à¸„à¸¢à¸­à¸²à¸¨à¸±à¸¢à¸­à¸¢à¸¹à¹ˆ': 'Palo Alto, California\\nà¹€à¸¡à¸·à¸­à¸‡à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™\\nDobbs Ferry, New York\\nà¹€à¸¡à¸·à¸­à¸‡à¹€à¸à¸´à¸”',\n",
       " 'à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š Mark': \"I'm trying to make the world a more open place.\",\n",
       " 'à¸„à¸³à¸„à¸¡à¸—à¸µà¹ˆà¸Šà¸­à¸š': '\"Fortune favors the bold.\"\\n- Virgil, Aeneid X.284\\n\\n\"All children are artists. The problem is how to remain an artist once you grow up.\"\\n- Pablo Picasso\\n\\n\"Make things as simple as possible but no simpler.\"\\n- Albert Einstein'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from facebook_scraper import get_profile\n",
    "username = 'khlau_1012@hotmail.com'\n",
    "password = 'your_password'\n",
    "get_profile(\"zuck\", credentials=(username , password))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Resources:\n",
    "- [facebook-post-scraper](https://github.com/brutalsavage/facebook-post-scraper). Has comments. Uses Selenium.\n",
    "- [facebook-scraper-selenium](https://github.com/apurvmishra99/facebook-scraper-selenium). \"Scrape posts from any group or user into a .csv file without needing to - register for any API access\".\n",
    "- [Ultimate Facebook Scraper](https://github.com/harismuneer/Ultimate-Facebook-Scraper). \"Scrapes almost everything about a Facebook user's profile\". Uses Selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 : tweepy\n",
    "\n",
    "Install Command:\n",
    "- !pip install tweepy\n",
    "\n",
    "Description:\n",
    "- Allows you to scrape tweets and replies\n",
    "\n",
    "Info List:\n",
    "![tweepy_info](tweepy_info.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-4.14.0-py3-none-any.whl (98 kB)\n",
      "     ---------------------------------------- 0.0/98.5 kB ? eta -:--:--\n",
      "     ---- ----------------------------------- 10.2/98.5 kB ? eta -:--:--\n",
      "     ---- ----------------------------------- 10.2/98.5 kB ? eta -:--:--\n",
      "     ----------- -------------------------- 30.7/98.5 kB 187.9 kB/s eta 0:00:01\n",
      "     ----------- -------------------------- 30.7/98.5 kB 187.9 kB/s eta 0:00:01\n",
      "     --------------- ---------------------- 41.0/98.5 kB 151.3 kB/s eta 0:00:01\n",
      "     --------------- ---------------------- 41.0/98.5 kB 151.3 kB/s eta 0:00:01\n",
      "     ----------------------- -------------- 61.4/98.5 kB 182.2 kB/s eta 0:00:01\n",
      "     ----------------------------------- -- 92.2/98.5 kB 228.2 kB/s eta 0:00:01\n",
      "     -------------------------------------- 98.5/98.5 kB 245.9 kB/s eta 0:00:00\n",
      "Collecting oauthlib<4,>=3.2.0 (from tweepy)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "     ------------- ------------------------- 51.2/151.7 kB 1.3 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 92.2/151.7 kB 1.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ---- 133.1/151.7 kB 1.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ---- 133.1/151.7 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- - 143.4/151.7 kB 607.9 kB/s eta 0:00:01\n",
      "     ------------------------------------ 151.7/151.7 kB 645.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from tweepy) (2.31.0)\n",
      "Collecting requests-oauthlib<2,>=1.2.0 (from tweepy)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2023.11.17)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.2.2 requests-oauthlib-1.3.1 tweepy-4.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_token = \"\"\n",
    "access_token_secret = \"\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scraping a specific Twitter userâ€™s Tweets:\n",
    "username = 'jack'  # username handle\n",
    "count = 150        # recent 150 tweets\n",
    "\n",
    "try:     \n",
    " # Creation of query method using parameters\n",
    " tweets = tweepy.Cursor(api.user_timeline,id=username).items(count)\n",
    " \n",
    " # Pulling information from tweets iterable object\n",
    " tweets_list = [[tweet.created_at, tweet.id, tweet.text] for tweet in tweets]\n",
    " \n",
    " # Creation of dataframe from tweets list\n",
    " # Add or remove columns as you remove tweet information\n",
    " tweets_df = pd.DataFrame(tweets_list)\n",
    "except BaseException as e:\n",
    "  print('failed on_status,',str(e))\n",
    "  time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scraping specific search query\n",
    "text_query = '2020 US Election'\n",
    "count = 150\n",
    "try:\n",
    "    tweets = tweepy.Cursor(api.search_tweets, q=text_query).items(count)\n",
    " \n",
    " # Pulling information from tweets iterable object\n",
    "    tweets_list = [[tweet.created_at, tweet.id, tweet.text] for tweet in tweets]\n",
    " \n",
    " # Creation of dataframe from tweets list\n",
    " # Add or remove columns as you remove tweet information\n",
    "    tweets_df = pd.DataFrame(tweets_list)\n",
    " \n",
    "except BaseException as e:\n",
    "    print('failed on_status,',str(e))\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instaloader\n",
    "\n",
    "Install Command:\n",
    "- !pip3 install instaloader\n",
    "\n",
    "Description:\n",
    "- downloads public and private profiles, hashtags, user stories, feeds and saved media,\n",
    "- downloads comments, geotags and captions of each post,\n",
    "- automatically detects profile name changes and renames the target directory accordingly,\n",
    "- allows fine-grained customization of filters and where to store downloaded media,\n",
    "- automatically resumes previously-interrupted download iterations.\n",
    "\n",
    "instaloader [--comments] [--geotags]\n",
    "            [--stories] [--highlights] [--tagged] [--igtv]\n",
    "            [--login YOUR-USERNAME] [--fast-update]\n",
    "            profile | \"#hashtag\" | :stories | :feed | :saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting instaloader\n",
      "  Downloading instaloader-4.10.3.tar.gz (62 kB)\n",
      "     ---------------------------------------- 0.0/62.9 kB ? eta -:--:--\n",
      "     ------------ ------------------------- 20.5/62.9 kB 330.3 kB/s eta 0:00:01\n",
      "     ------------------------------ ------- 51.2/62.9 kB 525.1 kB/s eta 0:00:01\n",
      "     -------------------------------------- 62.9/62.9 kB 478.7 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests>=2.4 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from instaloader) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from requests>=2.4->instaloader) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from requests>=2.4->instaloader) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from requests>=2.4->instaloader) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kahhou_lau\\appdata\\local\\anaconda3\\envs\\ads\\lib\\site-packages (from requests>=2.4->instaloader) (2023.11.17)\n",
      "Building wheels for collected packages: instaloader\n",
      "  Building wheel for instaloader (setup.py): started\n",
      "  Building wheel for instaloader (setup.py): finished with status 'done'\n",
      "  Created wheel for instaloader: filename=instaloader-4.10.3-py3-none-any.whl size=64800 sha256=e74d85ff83c71111b3aa668358596577c4687edea9aaea36f36e26d5b308fc88\n",
      "  Stored in directory: c:\\users\\kahhou_lau\\appdata\\local\\pip\\cache\\wheels\\79\\98\\b7\\4c15fe6680cf0e460b20fba742cf5052461e8b320f6f9f7e21\n",
      "Successfully built instaloader\n",
      "Installing collected packages: instaloader\n",
      "Successfully installed instaloader-4.10.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install instaloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['https://scontent-kul2-1.cdninstagram.com/v/t39.30808-6/413853325_18303547231130251_7778221327299527502_n.jpg?stp=dst-jpg_e15&_nc_ht=scontent-kul2-1.cdninstagram.com&_nc_cat=107&_nc_ohc=J8hzRc5wg0gAX8PCUHA&edm=AOQ1c0wAAAAA&ccb=7-5&ig_cache_key=MzI2NjAxMzE0MTAzNzEwOTc0NA%3D%3D.2-ccb7-5&oh=00_AfBSkMQoJ4vlUth3mpq9g1VA5nYzxQLUKOmvTR6YE3f0zg&oe=65A6E3EF&_nc_sid=8b3546', 'https://scontent-kul2-1.cdninstagram.com/v/t39.30808-6/394258941_18294093484130251_2272600211854056886_n.jpg?stp=dst-jpg_e35_p1080x1080_sh0.08&_nc_ht=scontent-kul2-1.cdninstagram.com&_nc_cat=107&_nc_ohc=AneGWvm0aGEAX_cs4iL&edm=AOQ1c0wAAAAA&ccb=7-5&ig_cache_key=MzIxODMwNTAwMTk2OTg3NzkwOA%3D%3D.2-ccb7-5&oh=00_AfCZ1dXYlm1RZe2cRn_RTAbimZLU2BBiZd0aX2Zj9_d4qw&oe=65A73A93&_nc_sid=8b3546'], 'https://scontent-kul2-1.cdninstagram.com/v/t51.2885-19/330996695_872055110760217_9044841881213205121_n.jpg?stp=dst-jpg_s320x320&_nc_ht=scontent-kul2-1.cdninstagram.com&_nc_cat=101&_nc_ohc=J7JYdllicOoAX-HyR9P&edm=AOQ1c0wBAAAA&ccb=7-5&oh=00_AfAFoV93FSktIfK77rOBTMBULB4odrit86ViA89Ua_7d_A&oe=65A61E86&_nc_sid=8b3546')\n"
     ]
    }
   ],
   "source": [
    "import instaloader\n",
    "import requests\n",
    "\n",
    "def get_instagram_pic_urls(username, max_count=2):\n",
    "    loader = instaloader.Instaloader()\n",
    "    profile = instaloader.Profile.from_username(loader.context, username)\n",
    "\n",
    "    image_name = \"ig_\" + username\n",
    "\n",
    "    image_urls = []\n",
    "\n",
    "    for i, post in enumerate(profile.get_posts()): \n",
    "        if i >= max_count:\n",
    "            break\n",
    "        image_url = post.url\n",
    "        image_urls.append(image_url)\n",
    "        # Optionally download the image in the same directory as the notebook\n",
    "        response = requests.get(image_url)\n",
    "        with open(image_name + str(i) + \".jpg\", \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "    \n",
    "    return image_urls, profile.profile_pic_url\n",
    "\n",
    "# Example usage\n",
    "print(get_instagram_pic_urls('brahderlau'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "L = instaloader.Instaloader()\n",
    "profile = instaloader.Profile.from_username(L.context, 'brahderlau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadCredentialsException",
     "evalue": "Login error: Wrong password.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadCredentialsException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m password \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBrahderlau99@\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43musername\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m instaloader\u001b[38;5;241m.\u001b[39mTwoFactorAuthRequiredException:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# The exception is raised if 2FA is required\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     verification_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter 2FA verification code: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Kahhou_Lau\\AppData\\Local\\anaconda3\\envs\\ads\\lib\\site-packages\\instaloader\\instaloader.py:651\u001b[0m, in \u001b[0;36mInstaloader.login\u001b[1;34m(self, user, passwd)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogin\u001b[39m(\u001b[38;5;28mself\u001b[39m, user: \u001b[38;5;28mstr\u001b[39m, passwd: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    644\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Log in to instagram with given username and password and internally store session object.\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \n\u001b[0;32m    646\u001b[0m \u001b[38;5;124;03m    :raises InvalidArgumentException: If the provided username does not exist.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;124;03m    :raises TwoFactorAuthRequiredException: First step of 2FA login done, now call\u001b[39;00m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;124;03m       :meth:`Instaloader.two_factor_login`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 651\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpasswd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kahhou_Lau\\AppData\\Local\\anaconda3\\envs\\ads\\lib\\site-packages\\instaloader\\instaloadercontext.py:306\u001b[0m, in \u001b[0;36mInstaloaderContext.login\u001b[1;34m(self, user, passwd)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resp_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthenticated\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resp_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;66;03m# '{\"authenticated\": false, \"user\": true, \"status\": \"ok\"}'\u001b[39;00m\n\u001b[1;32m--> 306\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m BadCredentialsException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogin error: Wrong password.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;66;03m# '{\"authenticated\": false, \"user\": false, \"status\": \"ok\"}'\u001b[39;00m\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;66;03m# Raise InvalidArgumentException rather than BadCredentialException, because BadCredentialException\u001b[39;00m\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;66;03m# triggers re-asking of password in Instaloader.interactive_login(), which makes no sense if the\u001b[39;00m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;66;03m# username is invalid.\u001b[39;00m\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidArgumentException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogin error: User \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(user))\n",
      "\u001b[1;31mBadCredentialsException\u001b[0m: Login error: Wrong password."
     ]
    }
   ],
   "source": [
    "username = 'brahderlau'\n",
    "password = ''\n",
    "try:\n",
    "    L.login(username, password)\n",
    "except instaloader.TwoFactorAuthRequiredException:\n",
    "    # The exception is raised if 2FA is required\n",
    "    verification_code = input(\"Enter 2FA verification code: \")\n",
    "    L.two_factor_login(verification_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-26 02:52:07\n",
      "brahderlau\\2023-12-26_02-52-07_UTC.jpg [Merry Christmas ğŸ…] json \n",
      "2023-10-21 07:04:34\n",
      "brahderlau\\2023-10-21_07-04-34_UTC.jpg [Unleash your style with Hermoâ€¦] json \n",
      "2023-09-30 09:06:41\n",
      "brahderlau\\2023-09-30_09-06-41_UTC.jpg [Have you rewarded yourself wiâ€¦] json \n",
      "2023-09-30 15:40:33\n",
      "brahderlau\\2023-09-30_15-40-33_UTC.jpg [Still eating MIXUE? Letâ€™s cheâ€¦] json \n",
      "2023-09-30 15:24:40\n",
      "brahderlau\\2023-09-30_15-24-40_UTC.jpg json \n",
      "2023-09-30 15:20:32\n",
      "brahderlau\\2023-09-30_15-20-32_UTC.jpg json \n",
      "2023-09-30 15:18:27\n",
      "brahderlau\\2023-09-30_15-18-27_UTC.jpg json \n",
      "2023-09-30 15:16:39\n",
      "brahderlau\\2023-09-30_15-16-39_UTC.jpg json \n",
      "2023-06-30 04:32:09\n",
      "brahderlau\\2023-06-30_04-32-09_UTC.jpg json \n",
      "2023-06-30 04:31:53\n",
      "brahderlau\\2023-06-30_04-31-53_UTC.jpg json \n",
      "2023-06-30 04:31:33\n",
      "brahderlau\\2023-06-30_04-31-33_UTC.jpg json \n",
      "2023-04-21 13:23:16\n",
      "brahderlau\\2023-04-21_13-23-16_UTC.jpg [Selamat Hari Raya Aidilfitri â€¦] json \n",
      "2023-03-22 12:10:37\n",
      "brahderlau\\2023-03-22_12-10-37_UTC.jpg [Saya ingin mengambil kesempatâ€¦] json \n",
      "2023-02-18 00:45:02\n",
      "brahderlau\\2023-02-18_00-45-02_UTC.jpg [æˆ‘è¿˜æ˜¯å¤Ÿé«˜çš„ ğŸ˜¬] json \n",
      "2023-01-31 05:53:50\n",
      "brahderlau\\2023-01-31_05-53-50_UTC.jpg [LocationğŸ“: KSH Bahn Mi Yue Naâ€¦] brahderlau\\2023-01-31_05-53-50_UTC.mp4 json \n",
      "2023-01-23 14:45:29\n",
      "brahderlau\\2023-01-23_14-45-29_UTC.jpg [åˆäºŒ : ä¸çŸ¥ä¸è§‰è·Ÿå§å§æ’è¡«ï¼Œå°±å½“ä½œæ˜¯æˆ‘å§å§çš„ä¸€å¤©å‡ºç§Ÿç”·å‹â€¦] json \n",
      "2023-01-23 14:36:46\n",
      "brahderlau\\2023-01-23_14-36-46_UTC.jpg [è¿™æ ·æ­é… ä¹Ÿå…”hypeäº†å§  #hypemalaysia â€¦] json \n",
      "2023-01-23 09:29:42\n",
      "brahderlau\\2023-01-23_09-29-42_UTC.jpg [ä¸¤åªè€å…”ğŸ°ğŸ°ï¼Œä¸€èµ·å…”twoå…”ğŸ¤©  #hypemalaysiâ€¦] brahderlau\\2023-01-23_09-29-42_UTC.mp4 json \n",
      "2023-01-22 07:52:08\n",
      "brahderlau\\2023-01-22_07-52-08_UTC.jpg [ä¸ä¸€æ ·é¢œè‰²çš„æœè£… æ„å‘³ç€æˆ‘ä»¬äº’ç›¸å¼¥è¡¥è‡ªå·±çš„ç¼ºç‚¹ è¿™æ ·æ‰æ˜¯ä¸€â€¦] json \n",
      "2023-01-22 07:02:29\n",
      "brahderlau\\2023-01-22_07-02-29_UTC.jpg [2023èƒ–å…”å…”çš„æˆ‘ğŸ°] json \n",
      "2023-01-22 04:15:53\n",
      "brahderlau\\2023-01-22_04-15-53_UTC.jpg [Happy Chinese New Year of Rabâ€¦] json \n",
      "2023-01-21 16:50:41\n",
      "brahderlau\\2023-01-21_16-50-41_UTC_1.jpg brahderlau\\2023-01-21_16-50-41_UTC_2.jpg brahderlau\\2023-01-21_16-50-41_UTC_3.jpg [é™¤å¤• - ç®€å•åˆç¾å‘³çš„ä½³è‚´ å›å‘³æ— ç©· ä¹Ÿç¥çˆ·çˆ·å…«åå››å¤§å¯¿ â€¦] json \n",
      "2023-01-21 15:54:12\n",
      "brahderlau\\2023-01-21_15-54-12_UTC.jpg [é™¤å¤•å‰ä¸€å¤œ, ä¸€å¹´ä¸€åº¦çš„è€åŒå­¦èšä¼šğŸ«ğŸ˜ƒ] json \n",
      "2023-01-21 15:45:03\n",
      "brahderlau\\2023-01-21_15-45-03_UTC.jpg [Why do they have so many hateâ€¦] json \n",
      "2023-01-20 02:48:19\n",
      "brahderlau\\2023-01-20_02-48-19_UTC.jpg [Is it logical to buy a house â€¦] json \n",
      "2023-01-18 14:12:07\n",
      "brahderlau\\2023-01-18_14-12-07_UTC_1.jpg brahderlau\\2023-01-18_14-12-07_UTC_2.jpg [[ ğ‡ğ¨ğ° ğ­ğ¨ ğ«ğğğ®ğœğ ğƒğ„ğğ“ â‰ï¸ ]  Thâ€¦] json \n",
      "2023-01-17 14:42:02\n",
      "brahderlau\\2023-01-17_14-42-02_UTC_1.jpg brahderlau\\2023-01-17_14-42-02_UTC_2.jpg [[ ğ‡ğ¨ğ° ğ­ğ¨ ğ¢ğ§ğœğ«ğğšğ¬ğ ğ¬ğšğ¯ğ¢ğ§ğ ğ¬ ğ›ğ² â€¦] json \n",
      "2023-01-16 11:24:18\n",
      "brahderlau\\2023-01-16_11-24-18_UTC_1.jpg brahderlau\\2023-01-16_11-24-18_UTC_2.jpg brahderlau\\2023-01-16_11-24-18_UTC_3.jpg [[ğ„ğ±ğ©ğğ«ğ¢ğğ§ğœğ ğ‰ğ¨ğ¢ğ§ğ¢ğ§ğ  ğ’ğ®ğ§ğğšğ² ğ’ğâ€¦] json \n",
      "2023-01-15 07:29:43\n",
      "brahderlau\\2023-01-15_07-29-43_UTC.jpg [ğ”ğğƒğ€ğ“ğ„ ğŸ‘: ğ…ğğğğ›ğšğœğ¤ ğŸğ«ğ¨ğ¦ ğ¨ğ­ğ¡ğğ«â€¦] json \n",
      "2023-01-14 15:47:11\n",
      "brahderlau\\2023-01-14_15-47-11_UTC_1.jpg brahderlau\\2023-01-14_15-47-11_UTC_2.jpg brahderlau\\2023-01-14_15-47-11_UTC_3.jpg brahderlau\\2023-01-14_15-47-11_UTC_4.jpg brahderlau\\2023-01-14_15-47-11_UTC_5.jpg brahderlau\\2023-01-14_15-47-11_UTC_6.jpg brahderlau\\2023-01-14_15-47-11_UTC_7.jpg brahderlau\\2023-01-14_15-47-11_UTC_8.jpg [ğ”ğğƒğ€ğ“ğ„ ğŸ: ğ„ğ±ğ©ğğ«ğ¢ğğ§ğœğ ğ£ğ¨ğ¢ğ§ğ¢ğ§ğ  â€¦] json \n",
      "2023-01-14 09:17:43\n",
      "brahderlau\\2023-01-14_09-17-43_UTC_1.jpg brahderlau\\2023-01-14_09-17-43_UTC_2.jpg brahderlau\\2023-01-14_09-17-43_UTC_3.jpg brahderlau\\2023-01-14_09-17-43_UTC_4.jpg brahderlau\\2023-01-14_09-17-43_UTC_5.jpg brahderlau\\2023-01-14_09-17-43_UTC_6.jpg brahderlau\\2023-01-14_09-17-43_UTC_7.jpg brahderlau\\2023-01-14_09-17-43_UTC_8.jpg [[ğğ®ğğ¬ğ­ğ¢ğ¨ğ§ ğ¨ğŸ ğ­ğ¡ğ ğğšğ²: ğ…ğ€ğ‘ ğ‚ğšğ©â€¦] json \n",
      "2023-01-01 07:38:34\n",
      "brahderlau\\2023-01-01_07-38-34_UTC.jpg [Thank you everyone being partâ€¦] brahderlau\\2023-01-01_07-38-34_UTC.mp4 json \n",
      "2022-12-25 15:56:02\n",
      "brahderlau\\2022-12-25_15-56-02_UTC.jpg [Letâ€™s celebrate Merry Christmâ€¦] json \n",
      "2022-12-25 15:17:50\n",
      "brahderlau\\2022-12-25_15-17-50_UTC.jpg [SantağŸ…when looking for the baâ€¦] json \n",
      "2022-12-25 14:59:19\n",
      "brahderlau\\2022-12-25_14-59-19_UTC.jpg [Santa Claus robbing the kids â€¦] json \n",
      "2022-12-25 14:36:11\n",
      "brahderlau\\2022-12-25_14-36-11_UTC.jpg [HO HO HO ğŸ…â¤ï¸] brahderlau\\2022-12-25_14-36-11_UTC.mp4 json \n",
      "2022-12-13 12:38:28\n",
      "brahderlau\\2022-12-13_12-38-28_UTC.jpg [Follow trend ğŸ˜¬ malu tapi mahuâ€¦] brahderlau\\2022-12-13_12-38-28_UTC.mp4 json \n",
      "2022-12-05 15:07:36\n",
      "brahderlau\\2022-12-05_15-07-36_UTC.jpg [éƒ½ä¸æ‡‚åœ¨æŒ‡ä»€ä¹ˆï¼Œçœ‹ä»€ä¹ˆğŸ¤”] json \n",
      "2022-12-04 14:02:14\n",
      "brahderlau\\2022-12-04_14-02-14_UTC.jpg brahderlau\\2022-12-04_14-02-14_UTC.mp4 json \n",
      "2022-12-03 04:38:48\n",
      "brahderlau\\2022-12-03_04-38-48_UTC.jpg [ğŸ“ğŸ‘‰ğŸ»â˜ï¸] json \n",
      "2022-12-01 15:25:42\n",
      "brahderlau\\2022-12-01_15-25-42_UTC.jpg [Let me go ğŸ“âœŠğŸ»] json \n",
      "2022-11-30 05:56:32\n",
      "brahderlau\\2022-11-30_05-56-32_UTC.jpg [FlyingğŸ“â˜ï¸] json \n",
      "2022-11-29 15:11:59\n",
      "brahderlau\\2022-11-29_15-11-59_UTC.jpg [ğŸ“ğŸŒ¸] json \n",
      "2022-11-28 15:26:44\n",
      "brahderlau\\2022-11-28_15-26-44_UTC.jpg [Pool-walking at Manatidur ğŸ˜‡] brahderlau\\2022-11-28_15-26-44_UTC.mp4 json \n",
      "2022-11-19 13:41:39\n",
      "brahderlau\\2022-11-19_13-41-39_UTC.jpg [Still waiting the result for â€¦] json \n",
      "2022-11-19 08:39:47\n",
      "brahderlau\\2022-11-19_08-39-47_UTC.jpg [Saya sudah menunaikan kewajipâ€¦] json \n",
      "2022-11-19 08:22:14\n",
      "brahderlau\\2022-11-19_08-22-14_UTC.jpg [Setiap undian daripada anda aâ€¦] json \n",
      "2022-11-19 07:55:17\n",
      "brahderlau\\2022-11-19_07-55-17_UTC.jpg [PRU15 ğŸ‡²ğŸ‡¾âœ…   Saya sudah undi, â€¦] json \n",
      "2022-06-19 16:19:41\n",
      "brahderlau\\2022-06-19_16-19-41_UTC.jpg [Get tanned while sleeping on â€¦] json \n",
      "2022-06-18 12:31:06\n",
      "brahderlau\\2022-06-18_12-31-06_UTC.jpg [Trying to blend in with the câ€¦] json \n",
      "2022-05-01 15:49:30\n",
      "brahderlau\\2022-05-01_15-49-30_UTC.jpg [Selamat Hari Raya Aidilfitri,â€¦] json \n",
      "2022-03-13 04:27:17\n",
      "brahderlau\\2022-03-13_04-27-17_UTC.jpg [Looks like broccolis from far ğŸ˜¬] json \n",
      "2022-02-05 00:57:34\n",
      "brahderlau\\2022-02-05_00-57-34_UTC.jpg [TQ for the amazing digital arâ€¦] json \n",
      "2022-02-04 08:22:26\n",
      "brahderlau\\2022-02-04_08-22-26_UTC.jpg [å¤šå–„ç”¨èº«è¾¹çš„ç‰©ä½“å»å¼¥è¡¥è‡ªå·±çš„ç¼ºç‚¹ğŸ™‚] json \n",
      "2022-02-03 04:08:12\n",
      "brahderlau\\2022-02-03_04-08-12_UTC.jpg [æ ‘å¶æŠšæ‘¸æˆ‘çš„è„¸é¢Š å‡‰å¿«æ— æ¯” #åˆäºŒå›å¨˜å®¶â¤ï¸] json \n",
      "2021-12-31 13:58:40\n",
      "brahderlau\\2021-12-31_13-58-40_UTC.jpg [Last photo of 2021] json \n",
      "2021-11-05 07:27:28\n",
      "brahderlau\\2021-11-05_07-27-28_UTC.jpg [å“ä¸€å“èŒ¶çš„é¦™å‘³] brahderlau\\2021-11-05_07-27-28_UTC.mp4 json \n",
      "2021-10-12 17:38:30\n",
      "brahderlau\\2021-10-12_17-38-30_UTC_1.jpg brahderlau\\2021-10-12_17-38-30_UTC_2.jpg brahderlau\\2021-10-12_17-38-30_UTC_3.jpg [Happy 22nd Birthday to me ğŸ» Tâ€¦] json \n",
      "2021-10-04 11:53:17\n",
      "brahderlau\\2021-10-04_11-53-17_UTC.jpg brahderlau\\2021-10-04_11-53-17_UTC.mp4 json \n",
      "2021-08-14 14:22:56\n",
      "brahderlau\\2021-08-14_14-22-56_UTC.jpg [Love the filter :3] brahderlau\\2021-08-14_14-22-56_UTC.mp4 json \n",
      "2021-08-14 13:38:20\n",
      "brahderlau\\2021-08-14_13-38-20_UTC.jpg [å˜¿å˜¿] brahderlau\\2021-08-14_13-38-20_UTC.mp4 json \n",
      "2021-08-02 05:04:06\n",
      "brahderlau\\2021-08-02_05-04-06_UTC.jpg [2nd Dose Completed âœ…] json \n",
      "2021-07-12 07:49:13\n",
      "brahderlau\\2021-07-12_07-49-13_UTC.jpg [I am vaccinated!!!!!!!! NEXT â€¦] json \n",
      "2021-02-11 13:55:54\n",
      "brahderlau\\2021-02-11_13-55-54_UTC.jpg [æ–°å¹´å°è¯•æ–°å‘å‹ #é™¤å¤•å¿«ä¹ #ç‰›å¹´å¤§å‰ #ç‰›è½¬ä¹¾å¤] json \n",
      "2020-10-12 15:59:27\n",
      "brahderlau\\2020-10-12_15-59-27_UTC_1.jpg brahderlau\\2020-10-12_15-59-27_UTC_2.jpg brahderlau\\2020-10-12_15-59-27_UTC_3.jpg brahderlau\\2020-10-12_15-59-27_UTC_4.jpg brahderlau\\2020-10-12_15-59-27_UTC_5.jpg brahderlau\\2020-10-12_15-59-27_UTC_6.jpg [Thank you all of you for addiâ€¦] json \n",
      "2020-01-28 09:18:32\n",
      "brahderlau\\2020-01-28_09-18-32_UTC.jpg [åˆ«è·Ÿæˆ‘è™šä¼ªï¼Œæˆ‘æ‡’å¾—æ•·è¡ï¼ #chorsam #ootd #åˆä¸‰] json \n",
      "2020-01-26 07:36:48\n",
      "brahderlau\\2020-01-26_07-36-48_UTC.jpg [æ–°çš„ä¸€å¹´ï¼Œé™¤äº†ç¥å¤§å®¶æš´å¯Œä¹Ÿå¯ä»¥è¯•è¯•æŠ±æˆ‘ ğŸ™ˆ#choryeeâ€¦] json \n",
      "2020-01-25 04:36:46\n",
      "brahderlau\\2020-01-25_04-36-46_UTC.jpg [æ–°å¹´å¿«ä¹ï¼Œæˆ‘å¯¹è‡ªå·±è¯´ï¼Œå¸Œæœ›ä½ çœ¼é‡Œé•¿ç€å¤ªé˜³ï¼Œç¬‘é‡Œå…¨æ˜¯å¦è¡ã€‚#â€¦] json \n",
      "2020-01-18 13:18:57\n",
      "brahderlau\\2020-01-18_13-18-57_UTC.jpg [Okay kayo] json \n",
      "2019-12-10 03:35:44\n",
      "brahderlau\\2019-12-10_03-35-44_UTC_1.jpg brahderlau\\2019-12-10_03-35-44_UTC_2.jpg brahderlau\\2019-12-10_03-35-44_UTC_3.jpg [Terbaek Geng QuQu @ Vloft Gamâ€¦] json \n",
      "2019-12-10 02:59:02\n",
      "brahderlau\\2019-12-10_02-59-02_UTC.jpg [When was the last time you viâ€¦] json \n",
      "2019-09-04 12:50:37\n",
      "brahderlau\\2019-09-04_12-50-37_UTC.jpg [Don't study me. You won't graâ€¦] json \n",
      "2019-08-31 04:27:50\n",
      "brahderlau\\2019-08-31_04-27-50_UTC.jpg ['Sayangi Malaysiaku: Malaysiaâ€¦] json \n",
      "2018-11-17 18:17:50\n",
      "brahderlau\\2018-11-17_18-17-50_UTC.jpg [If you can't find the light iâ€¦] json \n",
      "2018-10-14 14:19:37\n",
      "brahderlau\\2018-10-14_14-19-37_UTC.jpg [ğŸ˜‚ğŸ˜‚ğŸ˜‚] json \n",
      "2018-10-13 14:15:14\n",
      "brahderlau\\2018-10-13_14-15-14_UTC.jpg [Thank You Everyone Who Made Mâ€¦] json \n",
      "2018-10-13 14:11:28\n",
      "brahderlau\\2018-10-13_14-11-28_UTC.jpg json \n",
      "2018-10-13 14:06:47\n",
      "brahderlau\\2018-10-13_14-06-47_UTC.jpg json \n",
      "2018-05-19 11:13:47\n",
      "brahderlau\\2018-05-19_11-13-47_UTC.jpg json \n",
      "2018-05-13 07:51:18\n",
      "brahderlau\\2018-05-13_07-51-18_UTC.jpg [#pondview #peace #brahderphotâ€¦] json \n",
      "2018-05-12 12:32:37\n",
      "brahderlau\\2018-05-12_12-32-37_UTC.jpg [Happy Mother's Day to all theâ€¦] json \n",
      "2018-05-11 11:46:59\n",
      "brahderlau\\2018-05-11_11-46-59_UTC.jpg [#oillamp #brahderphotography â€¦] json \n",
      "2018-04-25 15:55:05\n",
      "brahderlau\\2018-04-25_15-55-05_UTC.jpg [Do what is right, not what isâ€¦] json \n",
      "2018-04-07 09:11:54\n",
      "brahderlau\\2018-04-07_09-11-54_UTC.jpg [Life mission accomplished!!! â€¦] json \n",
      "2017-11-25 10:00:46\n",
      "brahderlau\\2017-11-25_10-00-46_UTC.jpg [I am not for sale, i deserve â€¦] json \n",
      "2017-11-08 12:48:13\n",
      "brahderlau\\2017-11-08_12-48-13_UTC.jpg [When the lecturer is absent bâ€¦] json \n",
      "2017-10-24 12:04:01\n",
      "brahderlau\\2017-10-24_12-04-01_UTC.jpg [è¿™é¦–æ­Œå…¶å®å°±æ˜¯ç»™æˆ‘æ•´ä½“ä¸Šä¸€ç§å¾ˆå¹³é™ï¼Œæ¸©æš–ï¼Œèˆ’æœçš„æ„Ÿè§‰ã€‚ä»–è®©â€¦] brahderlau\\2017-10-24_12-04-01_UTC.mp4 json \n",
      "2017-10-07 12:13:34\n",
      "brahderlau\\2017-10-07_12-13-34_UTC.jpg [Never loves anyone who treatsâ€¦] json \n",
      "2017-10-05 10:15:04\n",
      "brahderlau\\2017-10-05_10-15-04_UTC.jpg [It's not how much we have, buâ€¦] json \n",
      "2017-09-10 08:15:22\n",
      "brahderlau\\2017-09-10_08-15-22_UTC_1.jpg brahderlau\\2017-09-10_08-15-22_UTC_2.jpg [Love is like stock trading, yâ€¦] json \n",
      "2017-06-25 09:30:25\n",
      "brahderlau\\2017-06-25_09-30-25_UTC.jpg json \n",
      "2017-06-19 03:01:06\n",
      "brahderlau\\2017-06-19_03-01-06_UTC.jpg json \n",
      "2017-06-18 12:35:38\n",
      "brahderlau\\2017-06-18_12-35-38_UTC.jpg [Sunset of the day :)] json \n",
      "2017-06-13 10:37:31\n",
      "brahderlau\\2017-06-13_10-37-31_UTC.jpg [Hiiii Mum! Love u always :)] json \n",
      "2017-06-12 02:02:52\n",
      "brahderlau\\2017-06-12_02-02-52_UTC.jpg [Light from the Heaven! #Blessed] json \n",
      "2017-06-10 12:02:09\n",
      "brahderlau\\2017-06-10_12-02-09_UTC.jpg [#NoFilter #Natural #Mysteriouâ€¦] json \n",
      "2017-04-30 14:14:08\n",
      "brahderlau\\2017-04-30_14-14-08_UTC.jpg [Always look further. #candid] json \n",
      "2017-04-20 04:46:24\n",
      "brahderlau\\2017-04-20_04-46-24_UTC.jpg [#drumstickawesome #iamawesome] brahderlau\\2017-04-20_04-46-24_UTC.mp4 json \n",
      "2017-03-24 06:38:06\n",
      "brahderlau\\2017-03-24_06-38-06_UTC.jpg [Thank you eGG!!!!! Love ya #eâ€¦] brahderlau\\2017-03-24_06-38-06_UTC.mp4 json \n",
      "2017-03-23 09:07:41\n",
      "brahderlau\\2017-03-23_09-07-41_UTC.jpg [Hiiiii pindapanda :3 #360togeâ€¦] json \n",
      "2017-03-01 12:33:50\n",
      "brahderlau\\2017-03-01_12-33-50_UTC.jpg [Sea view :)] json \n",
      "2017-02-28 04:48:16\n",
      "brahderlau\\2017-02-28_04-48-16_UTC.jpg [Hi Guys! This is my first newâ€¦] brahderlau\\2017-02-28_04-48-16_UTC.mp4 json \n",
      "2017-01-28 06:44:52\n",
      "brahderlau\\2017-01-28_06-44-52_UTC.jpg [#choryat] json \n",
      "2016-11-30 03:14:30\n",
      "brahderlau\\2016-11-30_03-14-30_UTC.jpg [Last day wearing school unifoâ€¦] json \n",
      "2016-10-24 12:08:58\n",
      "brahderlau\\2016-10-24_12-08-58_UTC.jpg [#me I choose bear bear :3 #brâ€¦] json \n",
      "2016-10-16 13:02:10\n",
      "brahderlau\\2016-10-16_13-02-10_UTC.jpg [Fate brought us together , weâ€¦] json \n"
     ]
    }
   ],
   "source": [
    "# Download Posts in a Specific Period\n",
    "from datetime import datetime\n",
    "from itertools import dropwhile, takewhile\n",
    "\n",
    "import instaloader\n",
    "\n",
    "L = instaloader.Instaloader()\n",
    "\n",
    "posts = instaloader.Profile.from_username(L.context, \"brahderlau\").get_posts()\n",
    "\n",
    "SINCE = datetime(2024, 1, 1)\n",
    "UNTIL = datetime(2024, 1, 12)\n",
    "\n",
    "for post in takewhile(lambda p: p.date <= UNTIL, dropwhile(lambda p: p.date > SINCE, posts)):\n",
    "    print(post.date)\n",
    "    L.download_post(post, \"brahderlau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching likes of all posts of profile realdonaldtrump.\n",
      "<Post C2BRI_QsPyp>\n"
     ]
    },
    {
     "ename": "LoginRequiredException",
     "evalue": "--login required to access likes of a post.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLoginRequiredException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m post \u001b[38;5;129;01min\u001b[39;00m profile\u001b[38;5;241m.\u001b[39mget_posts():\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(post)\n\u001b[1;32m---> 16\u001b[0m     likes \u001b[38;5;241m=\u001b[39m likes \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_likes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# | means union\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching followers of profile \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(profile\u001b[38;5;241m.\u001b[39musername))\n\u001b[0;32m     19\u001b[0m followers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(profile\u001b[38;5;241m.\u001b[39mget_followers())\n",
      "File \u001b[1;32mc:\\Users\\Kahhou_Lau\\AppData\\Local\\anaconda3\\envs\\ads\\lib\\site-packages\\instaloader\\structures.py:685\u001b[0m, in \u001b[0;36mPost.get_likes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;124;03mIterate over all likes of the post. A :class:`Profile` instance of each likee is yielded.\u001b[39;00m\n\u001b[0;32m    680\u001b[0m \n\u001b[0;32m    681\u001b[0m \u001b[38;5;124;03m.. versionchanged:: 4.5.4\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;124;03m   Require being logged in (as required by Instagram).\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mis_logged_in:\n\u001b[1;32m--> 685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LoginRequiredException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--login required to access likes of a post.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;66;03m# Avoid doing additional requests if there are no comments\u001b[39;00m\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mLoginRequiredException\u001b[0m: --login required to access likes of a post."
     ]
    }
   ],
   "source": [
    "# Get likes of a profile / Ghost followers (Fake Accounts)\n",
    "import instaloader\n",
    "\n",
    "L = instaloader.Instaloader()\n",
    "\n",
    "USER = \"realdonaldtrump\"\n",
    "PROFILE = USER\n",
    "\n",
    "\n",
    "profile = instaloader.Profile.from_username(L.context, PROFILE)\n",
    "\n",
    "likes = set()\n",
    "print(\"Fetching likes of all posts of profile {}.\".format(profile.username))\n",
    "for post in profile.get_posts():\n",
    "    print(post)\n",
    "    likes = likes | set(post.get_likes()) # | means union\n",
    "\n",
    "print(\"Fetching followers of profile {}.\".format(profile.username))\n",
    "followers = set(profile.get_followers())\n",
    "\n",
    "ghosts = followers - likes\n",
    "\n",
    "print(\"Storing ghosts into file.\")\n",
    "with open(\"inactive-users.txt\", 'w') as f:\n",
    "    for ghost in ghosts:\n",
    "        print(ghost.username, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Getting Top X Posts of a user (most liked)\n",
    "\n",
    "# from itertools import islice\n",
    "# from math import ceil\n",
    "\n",
    "# from instaloader import Instaloader, Profile\n",
    "\n",
    "# PROFILE = 'jyjosephine'        # profile to download from\n",
    "# X_percentage = 10    # percentage of posts that should be downloaded\n",
    "\n",
    "# L = Instaloader()\n",
    "\n",
    "# profile = Profile.from_username(L.context, PROFILE)\n",
    "# posts_sorted_by_likes = sorted(profile.get_posts(),\n",
    "#                                key=lambda p: p.likes + p.comments,\n",
    "#                                reverse=True)\n",
    "\n",
    "\n",
    "# for post in islice(posts_sorted_by_likes, ceil(profile.mediacount * X_percentage / 100)):\n",
    "#     L.download_post(post, PROFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: PRAW\n",
    "\n",
    "Install Command:\n",
    "- pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id='mTF5k367VUFFmQ', client_secret='mjhD5xlOjtzDM8uovN9mzkhVOC2KbA', user_agent='User-Agent: Mozilla/5.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # get 10 hot posts from the MachineLearning subreddit\n",
    "# hot_posts = reddit.subreddit('Coronavirus').hot(limit=10)\n",
    "# for post in hot_posts:\n",
    "#     print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # get hottest posts from all subreddits\n",
    "# hot_posts = reddit.subreddit('all').hot(limit=10)\n",
    "# for post in hot_posts:\n",
    "#     print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# posts = []\n",
    "# ml_subreddit = reddit.subreddit('Coronavirus')\n",
    "# for post in ml_subreddit.hot(limit=10):\n",
    "#     posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "# posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # get Coronavirus subreddit data\n",
    "# ml_subreddit = reddit.subreddit('Coronavirus')\n",
    "\n",
    "# print(ml_subreddit.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = reddit.submission(url=\"https://www.reddit.com/r/MapPorn/comments/a3p0uq/an_image_of_gps_tracking_of_multiple_wolves_in/\")\n",
    "# or \n",
    "# submission = reddit.submission(id=\"a3p0uq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for top_level_comment in submission.comments:\n",
    "#     print(top_level_comment.body)\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from praw.models import MoreComments\n",
    "\n",
    "# for top_level_comment in submission.comments:\n",
    "#     if isinstance(top_level_comment, MoreComments):\n",
    "#         continue\n",
    "#     print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=0)\n",
    "for top_level_comment in submission.comments:\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.comments.replace_more(limit=0)\n",
    "for comment in submission.comments.list():\n",
    "    print(comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRON JOB\n",
    "\n",
    "What is CRON?\n",
    "\n",
    "The system utility cron can be used to schedule programs to run automatically at predetermined intervals.\n",
    "\n",
    "Cron is a standard Unix utility that is used to schedule commands for automatic execution at specific intervals. For instance, you might have a script that produces web statistics that you want to run once a day automatically at 5:00 AM.\n",
    "\n",
    "Commands involving cron are referred to as \"cron jobs.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo we gonna open a Python based GUI that displays \"Hello World\"\n",
    "- pip install tk\n",
    "- Create a python file called \"hello_world.py\"\n",
    "- Remember the path you save it at \"E:/hello_world.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tk\n",
      "  Downloading tk-0.1.0-py3-none-any.whl (3.9 kB)\n",
      "Installing collected packages: tk\n",
      "Successfully installed tk-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tkinter as tk \n",
    "\n",
    "root= tk.Tk() \n",
    " \n",
    "canvas1 = tk.Canvas(root, width = 300, height = 300)\n",
    "canvas1.pack()\n",
    "\n",
    "label1 = tk.Label(root, text='Hello World!')\n",
    "canvas1.create_window(150, 150, window=label1)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For MacOS/Ubuntu/Linux-based systems (Usually in production)\n",
    "\n",
    "To open CRON, simply use the following commands\n",
    "```\n",
    "crontab -e\n",
    "```\n",
    "To check status for any errors, stop or start CRON service\n",
    "```\n",
    "service cron status\n",
    "service cron stop\n",
    "service cron start\n",
    "```\n",
    "\n",
    "The following is the command you want to run in CRON\n",
    "\n",
    "```\n",
    "30 3 * * * python /usr/home/hello_world.py\n",
    "```\n",
    "\n",
    "To learn more about the timing notation, here is a fantastic resource:\n",
    "https://crontab.guru/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Windows (Extra Knowledge)\n",
    "\n",
    "Step-1: Prepare the Python Script\n",
    "\n",
    "Step-2: Create Batch File to Run the Python Script\n",
    "\n",
    "- \"Path where your Python exe is stored\\python.exe\" \"Path where your Python script is stored\\script name.py\"\n",
    "\n",
    "```bat\n",
    "\"C:\\\\Users\\\\daffy\\\\Anaconda3\\\\python.exe\" \"E:\\\\hello_world.py\"\n",
    "pause\n",
    "```\n",
    "- Name the file 'run_python.bat'\n",
    "\n",
    "Step-3: Schedule the Python Script using Windows Scheduler\n",
    "- First, open the Control Panel and then click on the Administrative Tools:\n",
    "- Next, double-click on the Task Scheduler, and then choose the option to â€˜Create Basic Taskâ€¦â€™\n",
    "- Type a name for your task (you can also type a description if needed), and then press Next.\n",
    "- Next, I chose to start the task â€˜Dailyâ€™ since we wish to run the Python script daily at 6am:\n",
    "- The action will then recur everyday at 6am, staring from 2019-04-01. You can adjust those timing parameters to suit your needs.\n",
    "- Select, Start a program, and then press Next:\n",
    "- Next, use the Browse button to find the batch file that runs the Python script. In my case, I placed the Run_Python_Script batch file on my Desktop:\n",
    "- Finally, click on Finish, and you should be good to go:\n",
    "- From this point onward, youâ€™ll be greeted with â€˜Hello World!â€™ everyday at 6am:\n",
    "- Please note that there is more than one way to execute Python Script using the Windows Scheduler. I chose to create a batch file to run the Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
